"""
Crawler Agent - çˆ¬è™«æ™ºèƒ½ä½“
ä¸“é—¨è´Ÿè´£ç½‘ç»œçˆ¬è™«ä»»åŠ¡ï¼Œæœç´¢ MP3 é“¾æ¥ã€å›¾ç‰‡ã€è§†é¢‘ç­‰èµ„æº
ä½¿ç”¨å›½å†…å¯ç”¨çš„éŸ³ä¹æºï¼Œå¹¶è§£æçœŸå® MP3 é“¾æ¥
æ”¯æŒ Playwright åŠ¨æ€åŠ è½½é¡µé¢çš„è§†é¢‘æå–
"""
import asyncio
import sys
import os
import re
import json
import ssl
import hashlib
import base64
import shutil
import urllib.request
import urllib.parse
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
from loguru import logger

from ..base import BaseAgent, Task, Message

try:
    from ...config import Settings
except ImportError:
    Settings = None

PLAYWRIGHT_AVAILABLE = False
try:
    from playwright.async_api import async_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    pass


@dataclass
class SearchResult:
    """æœç´¢ç»“æœ"""
    title: str
    url: str
    source: str
    quality: str = "unknown"
    size: str = "unknown"
    duration: str = "unknown"
    extra_info: Dict = field(default_factory=dict)


@dataclass
class CrawlTask:
    """çˆ¬è™«ä»»åŠ¡"""
    task_id: str
    keyword: str
    task_type: str  # mp3, video, image, etc.
    status: str = "pending"  # pending, running, completed, failed
    results: List[SearchResult] = field(default_factory=list)
    created_at: datetime = field(default_factory=datetime.now)
    completed_at: Optional[datetime] = None
    error_message: str = ""


class CrawlerAgent(BaseAgent):
    """
    çˆ¬è™«æ™ºèƒ½ä½“

    èƒ½åŠ›ï¼š
    - æœç´¢ MP3 éŸ³ä¹é“¾æ¥ï¼ˆä½¿ç”¨å›½å†…å¯ç”¨æºå¹¶è§£æçœŸå®é“¾æ¥ï¼‰
    - æœç´¢è§†é¢‘é“¾æ¥
    - æœç´¢å›¾ç‰‡èµ„æº
    - ç½‘é¡µå†…å®¹æŠ“å–
    - API æ•°æ®è·å–
    """
    
    PRIORITY = 4
    KEYWORD_MAPPINGS = {
        "æœç´¢": ("web_search", {}),
        "æœç´¢ä¸€ä¸‹": ("web_search", {}),
        "æŸ¥ä¸€ä¸‹": ("web_search", {}),
        "æŸ¥æŸ¥": ("web_search", {}),
        "æŸ¥æ‰¾": ("web_search", {}),
        "æŸ¥æ‰¾ä¸€ä¸‹": ("web_search", {}),
        "æœä¸€ä¸‹": ("web_search", {}),
        "å¸®æˆ‘æœ": ("web_search", {}),
        "å¸®æˆ‘æœç´¢": ("web_search", {}),
        "ç½‘ä¸Šæœ": ("web_search", {}),
        "ç½‘ä¸Šæœç´¢": ("web_search", {}),
        "ç™¾åº¦ä¸€ä¸‹": ("web_search", {}),
        "ç™¾åº¦æœç´¢": ("web_search", {}),
        "è°·æ­Œæœç´¢": ("web_search", {}),
        "å¿…åº”æœç´¢": ("web_search", {}),
        "æœç‹—æœç´¢": ("web_search", {}),
        "æœç´¢ç½‘é¡µ": ("web_search", {}),
        "æœç´¢ç½‘ç«™": ("web_search", {}),
        "æœç´¢èµ„æ–™": ("web_search", {}),
        "æœç´¢ä¿¡æ¯": ("web_search", {}),
        "æœç´¢æ–°é—»": ("web_search", {}),
        "æœç´¢æ–°é—»": ("web_search", {}),
        "æŸ¥æ–°é—»": ("web_search", {}),
        "çœ‹æ–°é—»": ("web_search", {}),
        "æœ€æ–°æ–°é—»": ("web_search", {}),
        "ä»Šæ—¥æ–°é—»": ("web_search", {}),
        "çƒ­ç‚¹æ–°é—»": ("web_search", {}),
        "çƒ­æœ": ("web_search", {}),
        "çƒ­æœæ¦œ": ("web_search", {}),
        "çƒ­æœè¯é¢˜": ("web_search", {}),
        "æŠ“å–ç½‘é¡µ": ("crawl_webpage", {}),
        "æŠ“å–ç½‘ç«™": ("crawl_webpage", {}),
        "è·å–ç½‘é¡µ": ("crawl_webpage", {}),
        "è¯»å–ç½‘é¡µ": ("crawl_webpage", {}),
        "æ‰“å¼€ç½‘é¡µ": ("crawl_webpage", {}),
        "è®¿é—®ç½‘é¡µ": ("crawl_webpage", {}),
        "ä¸‹è½½æ–‡ä»¶": ("file_download", {}),
        "ä¸‹è½½": ("file_download", {}),
        "å¸®æˆ‘ä¸‹è½½": ("file_download", {}),
        "ä¸‹è½½å›¾ç‰‡": ("file_download", {}),
        "ä¸‹è½½è§†é¢‘": ("file_download", {}),
        "æœç´¢å›¾ç‰‡": ("image_search", {}),
        "æœå›¾ç‰‡": ("image_search", {}),
        "æ‰¾å›¾ç‰‡": ("image_search", {}),
        "æŸ¥å›¾ç‰‡": ("image_search", {}),
        "æœç´¢è§†é¢‘": ("video_search", {}),
        "æœè§†é¢‘": ("video_search", {}),
        "æ‰¾è§†é¢‘": ("video_search", {}),
        "æŸ¥è§†é¢‘": ("video_search", {}),
        "æœç´¢MP3": ("search_mp3", {}),
        "æœMP3": ("search_mp3", {}),
        "æœç´¢éŸ³ä¹ä¸‹è½½": ("search_mp3", {}),
        "æœæ­Œä¸‹è½½": ("search_mp3", {}),
        "æå–é“¾æ¥": ("scrape_links", {}),
        "è·å–é“¾æ¥": ("scrape_links", {}),
        "æŠ“å–é“¾æ¥": ("scrape_links", {}),
        "æå–è§†é¢‘é“¾æ¥": ("scrape_video_links", {}),
        "è·å–è§†é¢‘é“¾æ¥": ("scrape_video_links", {}),
        "æŠ“å–è§†é¢‘é“¾æ¥": ("scrape_video_links", {}),
        "è·å–mp4": ("scrape_video_links", {}),
        "è·å–mp4é“¾æ¥": ("scrape_video_links", {}),
        "æå–mp4": ("scrape_video_links", {}),
        "æå–mp4é“¾æ¥": ("scrape_video_links", {}),
        "æŠ“å–mp4": ("scrape_video_links", {}),
        "æŠ“å–mp4é“¾æ¥": ("scrape_video_links", {}),
        "è§£æè§†é¢‘": ("scrape_video_links", {}),
        "è§£æè§†é¢‘é“¾æ¥": ("scrape_video_links", {}),
        "è§£æmp4": ("scrape_video_links", {}),
        "è§£æmp4é“¾æ¥": ("scrape_video_links", {}),
        "è·å–è§†é¢‘": ("scrape_video_links", {}),
        "æå–è§†é¢‘": ("scrape_video_links", {}),
        "æŠ“å–è§†é¢‘": ("scrape_video_links", {}),
        "çˆ¬å–è§†é¢‘": ("scrape_video_links", {}),
        "çˆ¬å–é“¾æ¥": ("scrape_links", {}),
        "çˆ¬å–mp4": ("scrape_video_links", {}),
        "çˆ¬å–ç½‘é¡µ": ("crawl_webpage", {}),
        "çˆ¬å–ç½‘ç«™": ("crawl_webpage", {}),
        "çˆ¬è™«": ("crawl_webpage", {}),
        "çˆ¬å–": ("crawl_webpage", {}),
        "æŠ“å–": ("crawl_webpage", {}),
        "è·å–": ("crawl_webpage", {}),
        "æ‰¹é‡çˆ¬å–": ("batch_scrape", {}),
        "æ‰¹é‡æŠ“å–": ("batch_scrape", {}),
        "æ‰¹é‡è·å–": ("batch_scrape", {}),
        "æ‰¹é‡æå–": ("batch_scrape", {}),
        "æ‰¹é‡ä¸‹è½½": ("batch_scrape", {}),
    }

    def __init__(self):
        super().__init__(
            name="crawler_agent",
            description="çˆ¬è™«æ™ºèƒ½ä½“ - è´Ÿè´£ç½‘ç»œèµ„æºæœç´¢å’ŒæŠ“å–"
        )

        # æ³¨å†Œèƒ½åŠ›
        self.register_capability(
            capability="search_web",
            description="æœç´¢äº’è”ç½‘è·å–ä¿¡æ¯ã€‚å½“ä½ æ— æ³•ç¡®å®šçš„ä¿¡æ¯ï¼ˆå¦‚å®æ—¶æ–°é—»ã€æœ€æ–°æ•°æ®ï¼‰ï¼Œå¯ä»¥ä½¿ç”¨æ­¤å·¥å…·æŸ¥è¯¢ã€‚",
            parameters={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "æœç´¢å…³é”®è¯æˆ–é—®é¢˜"
                    },
                    "location": {
                        "type": "string",
                        "description": "ä½ç½®ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚'åŒ—äº¬å¸‚æœé˜³åŒº'ï¼Œç”¨äºå‘¨è¾¹æœç´¢"
                    }
                },
                "required": ["query"]
            },
            category="search"
        )
        
        self.register_capability("mp3_search", "MP3æœç´¢")
        self.register_capability("video_search", "è§†é¢‘æœç´¢")
        self.register_capability("image_search", "å›¾ç‰‡æœç´¢")
        self.register_capability("web_crawl", "ç½‘é¡µçˆ¬å–")
        self.register_capability("api_fetch", "APIè·å–")

        # ä»»åŠ¡é˜Ÿåˆ—
        self.tasks: Dict[str, CrawlTask] = {}
        self.active_tasks: set = set()

        # ç”¨æˆ·ä»£ç†
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
        ]

        logger.info("ğŸ•·ï¸ çˆ¬è™«æ™ºèƒ½ä½“å·²åˆå§‹åŒ–")

    def _send_message_to_chat(self, message: str):
        """å‘é€å³æ—¶æ¶ˆæ¯åˆ°å¯¹è¯æ¡†"""
        try:
            from PyQt6.QtWidgets import QApplication
            app = QApplication.instance()
            if app:
                for widget in app.topLevelWidgets():
                    if hasattr(widget, 'chat_window'):
                        main_window = widget
                        if hasattr(main_window, 'chat_window'):
                            chat_window = main_window.chat_window
                            if hasattr(chat_window, 'signal_helper'):
                                chat_window.signal_helper.emit_append_message("assistant", message)
                                break
        except Exception as e:
            logger.warning(f"å‘é€æ¶ˆæ¯å¤±è´¥: {e}")

    def _get_headers(self, referer: str = "") -> Dict[str, str]:
        """è·å–è¯·æ±‚å¤´"""
        import random
        headers = {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        if referer:
            headers['Referer'] = referer
        return headers

    def _create_ssl_context(self):
        """åˆ›å»º SSL ä¸Šä¸‹æ–‡"""
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        return ssl_context

    async def execute_task(self, task: Task) -> Any:
        """
        æ‰§è¡Œçˆ¬è™«ä»»åŠ¡
        """
        task_type = task.type
        params = task.params

        logger.info(f"ğŸ•·ï¸ æ‰§è¡Œçˆ¬è™«ä»»åŠ¡: {task_type}")

        if task_type == "search_mp3":
            return await self._search_mp3(params)
        elif task_type == "search_video":
            return await self._search_video(params)
        elif task_type == "search_image":
            return await self._search_image(params)
        elif task_type == "crawl_webpage":
            return await self._crawl_webpage(params)
        elif task_type == "fetch_api":
            return await self._fetch_api(params)
        elif task_type == "get_task_status":
            return self._get_task_status(params)
        elif task_type == "get_task_results":
            return self._get_task_results(params)
        elif task_type in ("web_search", "search_web"):
            return await self._web_search(params)
        elif task_type == "general":
            return await self._handle_general(params)
        elif task_type == "scrape_links":
            return await self._scrape_links(params)
        elif task_type == "scrape_video_links":
            return await self._scrape_links(params)
        elif task_type == "scrape_m3u8_links":
            return await self._scrape_links(params)
        elif task_type == "extract_mp4_links":
            return await self._scrape_links(params)
        elif task_type == "extract_video_links":
            return await self._scrape_links(params)
        elif task_type == "scrape_mp4_links":
            return await self._scrape_links(params)
        elif task_type == "extract_page_links":
            return await self._extract_page_links(params)
        elif task_type == "scrape_page_links":
            return await self._extract_page_links(params)
        elif task_type == "batch_scrape":
            return await self._batch_scrape(params)
        elif task_type == "file_download":
            return await self._download_video(params)
        elif task_type == "agent_help":
            return self._get_help_info()
        else:
            return f"âŒ ä¸æ”¯æŒçš„çˆ¬è™«ä»»åŠ¡ç±»å‹: {task_type}"
    
    async def _download_video(self, params: Dict) -> str:
        """ä¸‹è½½è§†é¢‘ï¼šå…ˆæŠ“å–é“¾æ¥ï¼Œå†ä¸‹è½½"""
        url = params.get("url", "")
        
        if not url:
            return "âŒ è¯·æä¾›è§†é¢‘é¡µé¢ URL"
        
        # æ¸…ç† URL
        url = url.strip().strip('`').strip(',').strip()
        
        # å‘é€æç¤ºæ¶ˆæ¯
        self._send_message_to_chat(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½è§†é¢‘...\n\nğŸ”— URL: {url[:80]}{'...' if len(url) > 80 else ''}")
        
        logger.info(f"ğŸ“¥ ä¸‹è½½è§†é¢‘: {url}")
        
        # è·å–ä¸‹è½½ç›®å½•
        if Settings:
            settings = Settings()
            download_dir = str(settings.directory.get_download_dir())
        else:
            download_dir = os.path.join(os.getcwd(), "downloads")
        os.makedirs(download_dir, exist_ok=True)
        
        # ä¼˜å…ˆå°è¯•ä½¿ç”¨ yt-dlpï¼ˆæ”¯æŒæ›´å¤šç½‘ç«™å’ŒåŠ å¯†è§†é¢‘ï¼‰
        try:
            import subprocess
            import tempfile
            yt_dlp_path = shutil.which('yt-dlp')
            if yt_dlp_path:
                logger.info(f"ğŸ“¥ å°è¯•ä½¿ç”¨ yt-dlp ä¸‹è½½...")
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                # ä½¿ç”¨ä¸´æ—¶ç›®å½•ä¸‹è½½ï¼ˆé¿å…ä¸­æ–‡è·¯å¾„é—®é¢˜ï¼‰
                temp_dir = tempfile.gettempdir()
                temp_output = os.path.join(temp_dir, f"video_{timestamp}.%(ext)s")
                
                cmd = [
                    yt_dlp_path,
                    '-f', 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best',
                    '--merge-output-format', 'mp4',
                    '-o', temp_output,
                    '--no-playlist',
                    '--progress',
                    '--newline',
                    '--no-check-certificates',
                    url
                ]
                
                logger.info(f"ğŸ¬ æ‰§è¡Œå‘½ä»¤: yt-dlp -f best {url}")
                
                process = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.STDOUT
                )
                
                stdout, _ = await process.communicate()
                
                if process.returncode == 0:
                    # åœ¨ä¸´æ—¶ç›®å½•æŸ¥æ‰¾ä¸‹è½½çš„æ–‡ä»¶
                    for f in os.listdir(temp_dir):
                        if timestamp in f and f.endswith(('.mp4', '.mkv', '.webm')):
                            temp_filepath = os.path.join(temp_dir, f)
                            # ç§»åŠ¨åˆ°ç›®æ ‡ç›®å½•
                            final_filepath = os.path.join(download_dir, f)
                            try:
                                shutil.move(temp_filepath, final_filepath)
                                filepath = final_filepath
                            except Exception as move_err:
                                logger.warning(f"ç§»åŠ¨æ–‡ä»¶å¤±è´¥: {move_err}ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶")
                                filepath = temp_filepath
                            
                            size = os.path.getsize(filepath)
                            logger.info(f"âœ… yt-dlp ä¸‹è½½å®Œæˆ: {filepath} ({size / 1024 / 1024:.2f} MB)")
                            return f"âœ… è§†é¢‘ä¸‹è½½å®Œæˆï¼\n\nğŸ“ æ–‡ä»¶è·¯å¾„: {filepath}\nğŸ“Š æ–‡ä»¶å¤§å°: {size / 1024 / 1024:.2f} MB"
                    return f"âœ… yt-dlp ä¸‹è½½å®Œæˆ\n\nğŸ“ ä¸‹è½½ç›®å½•: {download_dir}\nğŸ“ ä¸´æ—¶ç›®å½•: {temp_dir}"
                else:
                    error_msg = stdout.decode('utf-8', errors='ignore')[-500:] if stdout else "æœªçŸ¥é”™è¯¯"
                    logger.warning(f"yt-dlp ä¸‹è½½å¤±è´¥: {error_msg}")
        except Exception as e:
            logger.warning(f"yt-dlp ä¸å¯ç”¨: {e}")
        
        # å›é€€åˆ° Playwright æŠ“å–
        logger.info("ğŸ“¥ yt-dlp å¤±è´¥ï¼Œå›é€€åˆ° Playwright æŠ“å–...")
        
        # 1. å…ˆæŠ“å–è§†é¢‘é“¾æ¥
        scrape_params = params.copy()
        scrape_params["link_type"] = "video"
        
        links_result = await self._scrape_links(scrape_params)
        
        # æ£€æŸ¥æ˜¯å¦è·å–åˆ°é“¾æ¥
        if links_result.startswith("âŒ"):
            return links_result
        
        # æå–é“¾æ¥
        import re
        links = re.findall(r'https?://[^\s]+', links_result)
        
        if not links:
            return f"âŒ æœªæ‰¾åˆ°å¯ä¸‹è½½çš„è§†é¢‘é“¾æ¥\n\n{links_result}"
        
        # åˆ†ç±»é“¾æ¥ï¼šä¼˜å…ˆ m3u8 > tsåˆ†ç‰‡ > mp4
        m3u8_links = [l for l in links if '.m3u8' in l.lower()]
        mp4_links = [l for l in links if '.mp4' in l.lower() and '.m3u8' not in l.lower()]
        ts_links = [l for l in links if '.ts' in l.lower() and '.m3u8' not in l.lower()]
        
        # è¿‡æ»¤æ‰å°æ–‡ä»¶ï¼ˆé¢„è§ˆ/å¹¿å‘Šï¼‰ï¼Œä¼˜å…ˆé€‰æ‹©é«˜æ¸…ç‰ˆæœ¬
        # è…¾è®¯è§†é¢‘: f112007 = 1080p, f2 = é¢„è§ˆ, gzc_1000xxx = ä¸åŒæ¸…æ™°åº¦
        # æ³¨æ„: MP4 é“¾æ¥å¯èƒ½æ˜¯å¹¿å‘Šï¼ŒTS åˆ†ç‰‡æ‰æ˜¯æ­£ç‰‡
        def get_quality_score(url):
            score = 0
            url_lower = url.lower()
            # é«˜æ¸…æ ‡è¯†
            if 'f112007' in url_lower or 'f1080' in url_lower:
                score += 1000
            elif 'f720' in url_lower:
                score += 700
            elif 'f480' in url_lower:
                score += 400
            # é¿å…é¢„è§ˆæ–‡ä»¶
            if '.f2.' in url_lower or '_f2' in url_lower:
                score -= 500
            # é¿å…å¹¿å‘Š
            if 'ad' in url_lower or 'promo' in url_lower:
                score -= 1000
            return score
        
        # æŒ‰è´¨é‡æ’åº MP4 é“¾æ¥
        if mp4_links:
            mp4_links = sorted(mp4_links, key=get_quality_score, reverse=True)
            logger.info(f"ğŸ“¥ MP4 é“¾æ¥æŒ‰è´¨é‡æ’åºï¼Œæœ€ä½³: {mp4_links[0][:80]}...")
        
        # è¿‡æ»¤æ¨æ–­çš„ m3u8ï¼ˆé€šå¸¸æ— æ•ˆï¼‰ï¼Œåªä¿ç•™ä»ç½‘ç»œè¯·æ±‚ç›´æ¥æ•è·çš„
        # æ¨æ–­çš„åœ°å€é€šå¸¸åŒ…å« index.m3u8, playlist.m3u8 æˆ–ä»¥ .m3u8 ç»“å°¾
        real_m3u8_links = [l for l in m3u8_links if 
                          'index.m3u8' not in l and 
                          'playlist.m3u8' not in l and
                          not l.rstrip('/').endswith('.m3u8')]
        
        # ä¼˜å…ˆä½¿ç”¨ TS åˆ†ç‰‡ï¼ˆæ­£ç‰‡å†…å®¹ï¼‰ï¼ŒMP4 å¯èƒ½æ˜¯å¹¿å‘Š
        if len(ts_links) >= 3:
            logger.info(f"ğŸ“¥ æ‰¾åˆ° {len(ts_links)} ä¸ª TS åˆ†ç‰‡ï¼ˆæ­£ç‰‡ï¼‰ï¼Œä¼˜å…ˆåˆå¹¶ä¸‹è½½")
            return await self._download_ts_segments(ts_links, links_result)
        # å…¶æ¬¡é€‰æ‹©çœŸå®æ•è·çš„ m3u8
        elif real_m3u8_links:
            video_url = real_m3u8_links[0]
            logger.info(f"ğŸ“¥ æ‰¾åˆ° m3u8 æµåª’ä½“é“¾æ¥")
        # é«˜è´¨é‡ mp4ï¼ˆå¯èƒ½æ˜¯å¹¿å‘Šï¼Œéœ€éªŒè¯ï¼‰
        elif mp4_links and get_quality_score(mp4_links[0]) > 0:
            video_url = mp4_links[0]
            logger.info(f"ğŸ“¥ æ‰¾åˆ°é«˜è´¨é‡ MP4 é“¾æ¥ï¼ˆæ³¨æ„ï¼šå¯èƒ½æ˜¯å¹¿å‘Šï¼‰")
        # å¦‚æœæœ‰å°‘é‡ TS åˆ†ç‰‡ï¼Œä¹Ÿå°è¯•åˆå¹¶
        elif ts_links:
            logger.info(f"ğŸ“¥ æ‰¾åˆ° {len(ts_links)} ä¸ª TS åˆ†ç‰‡ï¼Œå°è¯•åˆå¹¶ä¸‹è½½")
            return await self._download_ts_segments(ts_links, links_result)
        # æ™®é€š mp4
        elif mp4_links:
            video_url = mp4_links[0]
            logger.info(f"ğŸ“¥ æ‰¾åˆ° MP4 é“¾æ¥")
        # æœ€åå°è¯•æ¨æ–­çš„ m3u8ï¼ˆé€šå¸¸æ— æ•ˆï¼‰
        elif m3u8_links:
            video_url = m3u8_links[0]
            logger.info(f"ğŸ“¥ å°è¯•æ¨æ–­çš„ m3u8 é“¾æ¥ï¼ˆå¯èƒ½æ— æ•ˆï¼‰")
        else:
            video_url = links[0]
        
        logger.info(f"ğŸ“¥ å‡†å¤‡ä¸‹è½½: {video_url[:100]}...")
        
        # 2. ä¸‹è½½è§†é¢‘
        try:
            # è·å–ç”¨æˆ·é…ç½®çš„ä¸‹è½½ç›®å½•
            if Settings:
                settings = Settings()
                download_dir = str(settings.directory.get_download_dir())
            else:
                download_dir = os.path.join(os.getcwd(), "downloads")
            os.makedirs(download_dir, exist_ok=True)
            logger.info(f"ğŸ“¥ ä¸‹è½½ç›®å½•: {download_dir}")
            
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            if '.m3u8' in video_url.lower():
                filename = f"video_{timestamp}.m3u8"
            elif '.mp4' in video_url.lower():
                filename = f"video_{timestamp}.mp4"
            else:
                filename = f"video_{timestamp}.mp4"
            
            filepath = os.path.join(download_dir, filename)
            
            # ä¸‹è½½æ–‡ä»¶
            logger.info(f"ğŸ“¥ å¼€å§‹ä¸‹è½½åˆ°: {filepath}")
            
            # å¯¹äº m3u8 æµåª’ä½“ï¼Œä½¿ç”¨ ffmpeg ä¸‹è½½
            if '.m3u8' in video_url.lower():
                return await self._download_m3u8(video_url, filepath, links_result)
            
            req = urllib.request.Request(video_url, headers=self._get_headers())
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: urllib.request.urlopen(req, timeout=300, context=self._create_ssl_context())
            )
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = response.getheader('Content-Length')
            if file_size:
                file_size = int(file_size)
                logger.info(f"ğŸ“¥ æ–‡ä»¶å¤§å°: {file_size / 1024 / 1024:.2f} MB")
            
            # å†™å…¥æ–‡ä»¶
            with open(filepath, 'wb') as f:
                while True:
                    chunk = await loop.run_in_executor(None, response.read, 8192)
                    if not chunk:
                        break
                    f.write(chunk)
            
            actual_size = os.path.getsize(filepath)
            logger.info(f"âœ… ä¸‹è½½å®Œæˆ: {filepath} ({actual_size / 1024 / 1024:.2f} MB)")
            
            return f"âœ… è§†é¢‘ä¸‹è½½å®Œæˆï¼\n\nğŸ“ æ–‡ä»¶è·¯å¾„: {filepath}\nğŸ“Š æ–‡ä»¶å¤§å°: {actual_size / 1024 / 1024:.2f} MB\n\nğŸ”— åŸå§‹é“¾æ¥:\n{video_url[:100]}..."
            
        except Exception as e:
            logger.error(f"ä¸‹è½½å¤±è´¥: {e}")
            return f"âŒ ä¸‹è½½å¤±è´¥: {str(e)}\n\nğŸ”— æ‰¾åˆ°çš„è§†é¢‘é“¾æ¥:\n{links_result}"
    
    async def _download_m3u8(self, m3u8_url: str, filepath: str, links_result: str) -> str:
        """ä½¿ç”¨ ffmpeg ä¸‹è½½ m3u8 æµåª’ä½“"""
        import shutil
        
        # æ£€æŸ¥ ffmpeg æ˜¯å¦å¯ç”¨
        ffmpeg_path = shutil.which('ffmpeg')
        if not ffmpeg_path:
            return f"âš ï¸ m3u8 æµåª’ä½“éœ€è¦ ffmpeg æ”¯æŒ\n\nè¯·å…ˆå®‰è£… ffmpeg:\nâ€¢ Windows: winget install ffmpeg\nâ€¢ Mac: brew install ffmpeg\nâ€¢ Linux: apt install ffmpeg\n\nğŸ”— m3u8 é“¾æ¥:\n{m3u8_url[:100]}...\n\nğŸ“‹ æ‰€æœ‰æ‰¾åˆ°çš„é“¾æ¥:\n{links_result}"
        
        # ä¿®æ”¹è¾“å‡ºæ–‡ä»¶åä¸º mp4
        if filepath.endswith('.m3u8'):
            filepath = filepath[:-5] + '.mp4'
        
        logger.info(f"ğŸ“¥ ä½¿ç”¨ ffmpeg ä¸‹è½½ m3u8: {filepath}")
        
        try:
            import subprocess
            
            # æ„å»º ffmpeg å‘½ä»¤
            cmd = [
                ffmpeg_path,
                '-i', m3u8_url,
                '-c', 'copy',  # ç›´æ¥å¤åˆ¶æµï¼Œä¸é‡æ–°ç¼–ç 
                '-bsf:a', 'aac_adtstoasc',  # ä¿®å¤ AAC éŸ³é¢‘
                '-y',  # è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶
                filepath
            ]
            
            logger.info(f"ğŸ¬ æ‰§è¡Œå‘½ä»¤: ffmpeg -i {m3u8_url[:50]}... -c copy {filepath}")
            
            # æ‰§è¡Œå‘½ä»¤
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            
            if process.returncode == 0:
                actual_size = os.path.getsize(filepath)
                logger.info(f"âœ… m3u8 ä¸‹è½½å®Œæˆ: {filepath} ({actual_size / 1024 / 1024:.2f} MB)")
                return f"âœ… è§†é¢‘ä¸‹è½½å®Œæˆï¼\n\nğŸ“ æ–‡ä»¶è·¯å¾„: {filepath}\nğŸ“Š æ–‡ä»¶å¤§å°: {actual_size / 1024 / 1024:.2f} MB\n\nğŸ”— m3u8 é“¾æ¥:\n{m3u8_url[:100]}..."
            else:
                error_msg = stderr.decode('utf-8', errors='ignore')[-500:]
                logger.error(f"ffmpeg ä¸‹è½½å¤±è´¥: {error_msg}")
                return f"âŒ ffmpeg ä¸‹è½½å¤±è´¥\n\né”™è¯¯ä¿¡æ¯: {error_msg}\n\nğŸ”— m3u8 é“¾æ¥:\n{m3u8_url[:100]}...\n\nğŸ“‹ æ‰€æœ‰æ‰¾åˆ°çš„é“¾æ¥:\n{links_result}"
                
        except Exception as e:
            logger.error(f"m3u8 ä¸‹è½½å¤±è´¥: {e}")
            return f"âŒ m3u8 ä¸‹è½½å¤±è´¥: {str(e)}\n\nğŸ”— m3u8 é“¾æ¥:\n{m3u8_url[:100]}...\n\nğŸ“‹ æ‰€æœ‰æ‰¾åˆ°çš„é“¾æ¥:\n{links_result}"
    
    async def _download_ts_segments(self, ts_links: list, links_result: str) -> str:
        """ä¸‹è½½å¹¶åˆå¹¶ TS åˆ†ç‰‡"""
        from datetime import datetime
        import shutil
        
        # æ£€æŸ¥ ffmpeg æ˜¯å¦å¯ç”¨
        ffmpeg_path = shutil.which('ffmpeg')
        if not ffmpeg_path:
            return f"âš ï¸ TS åˆ†ç‰‡åˆå¹¶éœ€è¦ ffmpeg æ”¯æŒ\n\nè¯·å…ˆå®‰è£… ffmpeg:\nâ€¢ Windows: winget install ffmpeg\nâ€¢ Mac: brew install ffmpeg\nâ€¢ Linux: apt install ffmpeg\n\nğŸ“‹ æ‰¾åˆ° {len(ts_links)} ä¸ª TS åˆ†ç‰‡é“¾æ¥"
        
        # è·å–ç”¨æˆ·é…ç½®çš„ä¸‹è½½ç›®å½•
        if Settings:
            settings = Settings()
            download_dir = str(settings.directory.get_download_dir())
        else:
            download_dir = os.path.join(os.getcwd(), "downloads")
        os.makedirs(download_dir, exist_ok=True)
        logger.info(f"ğŸ“¥ ä¸‹è½½ç›®å½•: {download_dir}")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        temp_dir = os.path.join(download_dir, f"ts_temp_{timestamp}")
        os.makedirs(temp_dir, exist_ok=True)
        
        output_file = os.path.join(download_dir, f"video_{timestamp}.mp4")
        
        logger.info(f"ğŸ“¥ å¼€å§‹ä¸‹è½½ {len(ts_links)} ä¸ª TS åˆ†ç‰‡...")
        
        try:
            # æŒ‰ index æ’åº
            def get_index(url):
                match = re.search(r'index=(\d+)', url)
                if match:
                    return int(match.group(1))
                match = re.search(r'/(\d+)_', url)
                if match:
                    return int(match.group(1))
                return 0
            
            sorted_links = sorted(ts_links, key=get_index)
            
            # å»é‡ - æŒ‰ URL åŸºç¡€è·¯å¾„å»é‡
            seen_bases = set()
            unique_links = []
            for link in sorted_links:
                # æå–åŸºç¡€è·¯å¾„ï¼ˆä¸å« token ç­‰å‚æ•°ï¼‰
                base_match = re.match(r'(https?://[^?]+)', link)
                if base_match:
                    base = base_match.group(1)
                    # æå–åˆ†ç‰‡ç¼–å·
                    num_match = re.search(r'/(\d+)_gzc_', base)
                    if num_match:
                        num = num_match.group(1)
                        if num not in seen_bases:
                            seen_bases.add(num)
                            unique_links.append(link)
            
            sorted_links = unique_links if unique_links else sorted_links
            logger.info(f"ğŸ“¥ å»é‡å {len(sorted_links)} ä¸ªåˆ†ç‰‡")
            
            # ä¸‹è½½æ‰€æœ‰åˆ†ç‰‡
            downloaded_files = []
            loop = asyncio.get_event_loop()
            
            max_segments = 500  # å¢åŠ åˆ°500ä¸ªåˆ†ç‰‡
            for i, ts_url in enumerate(sorted_links[:max_segments]):
                try:
                    ts_file = os.path.join(temp_dir, f"segment_{i:04d}.ts")
                    
                    req = urllib.request.Request(ts_url, headers=self._get_headers())
                    response = await loop.run_in_executor(
                        None,
                        lambda u=ts_url: urllib.request.urlopen(
                            urllib.request.Request(u, headers=self._get_headers()),
                            timeout=30,
                            context=self._create_ssl_context()
                        )
                    )
                    
                    with open(ts_file, 'wb') as f:
                        f.write(response.read())
                    
                    downloaded_files.append(ts_file)
                    
                    if (i + 1) % 10 == 0:
                        logger.info(f"ğŸ“¥ å·²ä¸‹è½½ {i + 1}/{len(sorted_links[:max_segments])} ä¸ªåˆ†ç‰‡")
                        
                except Exception as e:
                    logger.warning(f"ä¸‹è½½åˆ†ç‰‡ {i} å¤±è´¥: {e}")
            
            if not downloaded_files:
                return f"âŒ æ‰€æœ‰ TS åˆ†ç‰‡ä¸‹è½½å¤±è´¥\n\nğŸ“‹ æ‰¾åˆ°çš„é“¾æ¥:\n{links_result}"
            
            logger.info(f"ğŸ“¥ ä¸‹è½½å®Œæˆï¼Œå…± {len(downloaded_files)} ä¸ªåˆ†ç‰‡ï¼Œå¼€å§‹åˆå¹¶...")
            
            # åˆ›å»ºåˆå¹¶åˆ—è¡¨æ–‡ä»¶
            list_file = os.path.join(temp_dir, "filelist.txt")
            with open(list_file, 'w', encoding='utf-8') as f:
                for ts_file in downloaded_files:
                    f.write(f"file '{ts_file}'\n")
            
            # ä½¿ç”¨ ffmpeg åˆå¹¶
            cmd = [
                ffmpeg_path,
                '-f', 'concat',
                '-safe', '0',
                '-i', list_file,
                '-c', 'copy',
                '-y',
                output_file
            ]
            
            logger.info(f"ğŸ¬ ä½¿ç”¨ ffmpeg åˆå¹¶åˆ†ç‰‡...")
            
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                shutil.rmtree(temp_dir)
            except:
                pass
            
            if process.returncode == 0:
                actual_size = os.path.getsize(output_file)
                logger.info(f"âœ… TS åˆå¹¶å®Œæˆ: {output_file} ({actual_size / 1024 / 1024:.2f} MB)")
                return f"âœ… è§†é¢‘ä¸‹è½½å®Œæˆï¼\n\nğŸ“ æ–‡ä»¶è·¯å¾„: {output_file}\nğŸ“Š æ–‡ä»¶å¤§å°: {actual_size / 1024 / 1024:.2f} MB\nğŸ“¦ åˆå¹¶åˆ†ç‰‡: {len(downloaded_files)} ä¸ª"
            else:
                error_msg = stderr.decode('utf-8', errors='ignore')[-500:]
                logger.error(f"ffmpeg åˆå¹¶å¤±è´¥: {error_msg}")
                return f"âŒ åˆ†ç‰‡åˆå¹¶å¤±è´¥\n\né”™è¯¯ä¿¡æ¯: {error_msg}"
                
        except Exception as e:
            logger.error(f"TS ä¸‹è½½åˆå¹¶å¤±è´¥: {e}")
            return f"âŒ TS åˆ†ç‰‡ä¸‹è½½åˆå¹¶å¤±è´¥: {str(e)}\n\nğŸ“‹ æ‰¾åˆ°çš„é“¾æ¥:\n{links_result}"
    
    async def _batch_scrape(self, params: Dict) -> str:
        """æ‰¹é‡çˆ¬å–è§†é¢‘é“¾æ¥"""
        url_template = params.get("url", "") or params.get("url_template", "")
        start_id = params.get("start_id", 0)
        end_id = params.get("end_id", 0)
        link_type = params.get("link_type", "mp4")
        
        # ä» URL ä¸­æå–æ¨¡æ¿å’Œ ID èŒƒå›´
        if url_template and not start_id:
            id_match = re.search(r'/(\d+)(?:[/\?]|$)', url_template)
            if id_match:
                start_id = int(id_match.group(1))
                # å¦‚æœæ²¡æœ‰æŒ‡å®šç»“æŸ IDï¼Œé»˜è®¤çˆ¬å– 10 ä¸ª
                if not end_id:
                    end_id = start_id + 9
        
        if not url_template or not start_id or not end_id:
            return "âŒ æ‰¹é‡çˆ¬å–éœ€è¦æä¾› URL æ¨¡æ¿å’Œ ID èŒƒå›´\n\næ ¼å¼ï¼šä» https://example.com/video/138009 åˆ° https://example.com/video/138200"
        
        if start_id > end_id:
            return f"âŒ èµ·å§‹ ID ({start_id}) ä¸èƒ½å¤§äºç»“æŸ ID ({end_id})"
        
        # é™åˆ¶æ‰¹é‡çˆ¬å–æ•°é‡
        max_count = 50
        if end_id - start_id + 1 > max_count:
            end_id = start_id + max_count - 1
            logger.warning(f"æ‰¹é‡çˆ¬å–æ•°é‡é™åˆ¶ä¸º {max_count} ä¸ª")
        
        # å‘é€æç¤ºæ¶ˆæ¯
        count = end_id - start_id + 1
        self._send_message_to_chat(f"ğŸ”„ å¼€å§‹æ‰¹é‡çˆ¬å–...\n\nğŸ“Š æ•°é‡: {count} ä¸ªé¡µé¢\nğŸ”— ä» {start_id} åˆ° {end_id}\nğŸ“ ç±»å‹: {link_type}")
        
        logger.info(f"ğŸ”„ æ‰¹é‡çˆ¬å–: {url_template} ä» {start_id} åˆ° {end_id}")
        
        # ç”Ÿæˆ URL åˆ—è¡¨
        base_url = re.sub(r'/\d+(/?)$', '/{id}\\1', url_template)
        if '{id}' not in base_url:
            base_url = url_template.rsplit('/', 1)[0] + '/{id}'
        
        all_results = []
        failed_ids = []
        
        for video_id in range(start_id, end_id + 1):
            url = base_url.replace('{id}', str(video_id))
            logger.info(f"ğŸ”„ çˆ¬å–: {url}")
            
            try:
                # ä½¿ç”¨ Playwright åŠ¨æ€æŠ“å–
                if PLAYWRIGHT_AVAILABLE:
                    links, error = await self._scrape_dynamic_video(url)
                    if links:
                        # è¿‡æ»¤ MP4 é“¾æ¥
                        if link_type.lower() == "mp4":
                            links = [l for l in links if '.mp4' in l.lower()]
                        elif link_type.lower() == "m3u8":
                            links = [l for l in links if '.m3u8' in l.lower()]
                        
                        if links:
                            all_results.append({
                                "id": video_id,
                                "url": url,
                                "links": links
                            })
                            continue
                
                # å¦‚æœ Playwright å¤±è´¥ï¼Œå°è¯•é™æ€æŠ“å–
                req = urllib.request.Request(url, headers=self._get_headers())
                loop = asyncio.get_event_loop()
                response = await loop.run_in_executor(
                    None,
                    lambda u=url: urllib.request.urlopen(
                        urllib.request.Request(u, headers=self._get_headers()),
                        timeout=30,
                        context=self._create_ssl_context()
                    )
                )
                html = response.read().decode('utf-8', errors='ignore')
                
                # æå–è§†é¢‘é“¾æ¥
                video_patterns = [
                    r'https?://[^\s<>"\']+\.(?:mp4|m3u8)[^\s<>"\']*',
                ]
                
                links = []
                for pattern in video_patterns:
                    matches = re.findall(pattern, html, re.IGNORECASE)
                    for m in matches:
                        m = m.strip('"\'')
                        if link_type.lower() == "mp4" and '.mp4' in m.lower():
                            links.append(m)
                        elif link_type.lower() == "m3u8" and '.m3u8' in m.lower():
                            links.append(m)
                        elif link_type.lower() not in ["mp4", "m3u8"]:
                            links.append(m)
                
                if links:
                    all_results.append({
                        "id": video_id,
                        "url": url,
                        "links": list(dict.fromkeys(links))
                    })
                else:
                    failed_ids.append(video_id)
                    
            except Exception as e:
                logger.warning(f"çˆ¬å– {url} å¤±è´¥: {e}")
                failed_ids.append(video_id)
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¢«å°
            await asyncio.sleep(1)
        
        # æ ¼å¼åŒ–è¾“å‡º
        if not all_results:
            return f"âŒ æ‰¹é‡çˆ¬å–å¤±è´¥ï¼Œå…± {len(failed_ids)} ä¸ªé¡µé¢æ— æ³•è®¿é—®"
        
        result_text = f"ğŸ”„ æ‰¹é‡çˆ¬å–å®Œæˆ\n\n"
        result_text += f"ğŸ“Š ç»Ÿè®¡ï¼š\n"
        result_text += f"â€¢ æˆåŠŸï¼š{len(all_results)} ä¸ª\n"
        result_text += f"â€¢ å¤±è´¥ï¼š{len(failed_ids)} ä¸ª\n"
        result_text += f"â€¢ ID èŒƒå›´ï¼š{start_id} - {end_id}\n\n"
        
        result_text += f"ğŸ¬ è§†é¢‘é“¾æ¥ï¼š\n\n"
        for item in all_results:
            result_text += f"ã€ID: {item['id']}ã€‘{item['url']}\n"
            for i, link in enumerate(item['links'][:3], 1):
                result_text += f"  {i}. {link}\n"
            if len(item['links']) > 3:
                result_text += f"  ... è¿˜æœ‰ {len(item['links']) - 3} ä¸ªé“¾æ¥\n"
            result_text += "\n"
        
        if failed_ids and len(failed_ids) <= 10:
            result_text += f"\nâš ï¸ å¤±è´¥çš„ IDï¼š{', '.join(map(str, failed_ids))}"
        
        return result_text
    
    async def _extract_page_links(self, params: Dict) -> str:
        """æå–ç½‘é¡µä¸­çš„æœ‰æ•ˆé“¾æ¥ï¼Œè¿‡æ»¤å¹¿å‘Šç­‰åƒåœ¾å†…å®¹"""
        url = params.get("url", "")
        
        if not url:
            return "âŒ è¯·æä¾›ç½‘é¡µ URL"
        
        # æ¸…ç† URL
        url = url.strip().strip('`').strip(',').strip()
        
        logger.info(f"ğŸ”— æå–é¡µé¢é“¾æ¥: {url}")
        
        # å¹¿å‘Šå’Œåƒåœ¾é“¾æ¥å…³é”®è¯
        ad_keywords = [
            'ad', 'ads', 'adv', 'advert', 'advertising', 'advertisement',
            'banner', 'popup', 'popunder', 'sponsor', 'sponsored',
            'analytics', 'tracking', 'tracker', 'pixel', 'beacon',
            'doubleclick', 'googlesyndication', 'googleadservices',
            'facebook.com/tr', 'facebook.com/plugins',
            'twitter.com/i/ads', 'linkedin.com/pixel',
            'criteo', 'outbrain', 'taboola', 'revcontent',
            'aff', 'affiliate', 'promo', 'promotion',
            'click', 'redirect', 'go.php', 'jump.php', 'link.php',
            'count', 'stat', 'tongji', 'cnzz', 'baidu.com/hm',
            'google-analytics', 'googletagmanager',
        ]
        
        # æ— æ•ˆé“¾æ¥åç¼€
        invalid_extensions = [
            '.css', '.js', '.json', '.xml', '.rss', '.atom',
            '.png', '.jpg', '.jpeg', '.gif', '.webp', '.svg', '.ico', '.bmp',
            '.woff', '.woff2', '.ttf', '.eot', '.otf',
            '.mp3', '.mp4', '.avi', '.mkv', '.mov', '.wmv', '.flv', '.webm',
            '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',
            '.zip', '.rar', '.7z', '.tar', '.gz',
        ]
        
        # æ— æ•ˆé“¾æ¥æ¨¡å¼
        invalid_patterns = [
            r'^javascript:', r'^mailto:', r'^tel:', r'^#',
            r'^data:', r'^blob:', r'^about:',
        ]
        
        # ä¼˜å…ˆä½¿ç”¨ Playwright åŠ¨æ€æŠ“å–
        if PLAYWRIGHT_AVAILABLE:
            logger.info(f"ğŸ­ ä½¿ç”¨ Playwright åŠ¨æ€æå–é¡µé¢é“¾æ¥...")
            dynamic_links = await self._extract_links_with_playwright(url, ad_keywords, invalid_extensions, invalid_patterns)
            if dynamic_links:
                # åˆ†ç±»é“¾æ¥
                from urllib.parse import urlparse
                base_domain = urlparse(url).netloc
                same_domain = []
                external_domain = []
                
                for link in dynamic_links:
                    parsed = urlparse(link)
                    if parsed.netloc == base_domain:
                        same_domain.append(link)
                    else:
                        external_domain.append(link)
                
                result_text = f"ğŸ”— ä» {url} æå–åˆ° {len(dynamic_links)} ä¸ªæœ‰æ•ˆé“¾æ¥ï¼š\n\n"
                
                if same_domain:
                    result_text += f"ğŸ“‚ ç«™å†…é“¾æ¥ ({len(same_domain)} ä¸ª)ï¼š\n"
                    for i, link in enumerate(same_domain[:15], 1):
                        result_text += f"  {i}. {link}\n"
                    if len(same_domain) > 15:
                        result_text += f"  ... è¿˜æœ‰ {len(same_domain) - 15} ä¸ª\n"
                    result_text += "\n"
                
                if external_domain:
                    result_text += f"ğŸŒ å¤–éƒ¨é“¾æ¥ ({len(external_domain)} ä¸ª)ï¼š\n"
                    for i, link in enumerate(external_domain[:10], 1):
                        result_text += f"  {i}. {link}\n"
                    if len(external_domain) > 10:
                        result_text += f"  ... è¿˜æœ‰ {len(external_domain) - 10} ä¸ª\n"
                
                return result_text
        
        try:
            req = urllib.request.Request(url, headers=self._get_headers())
            
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: urllib.request.urlopen(req, timeout=60, context=self._create_ssl_context())
            )
            
            html = response.read().decode('utf-8', errors='ignore')
            
            # æå–æ‰€æœ‰é“¾æ¥
            from urllib.parse import urljoin, urlparse
            
            all_links = []
            link_pattern = r'href=["\']([^"\']+)["\']'
            matches = re.findall(link_pattern, html, re.IGNORECASE)
            
            base_domain = urlparse(url).netloc
            
            for match in matches:
                match = match.strip()
                
                # è·³è¿‡æ— æ•ˆæ¨¡å¼
                if any(re.match(p, match, re.IGNORECASE) for p in invalid_patterns):
                    continue
                
                # è¡¥å…¨ç›¸å¯¹è·¯å¾„
                if not match.startswith('http'):
                    match = urljoin(url, match)
                
                # è§£æURL
                try:
                    parsed = urlparse(match)
                except:
                    continue
                
                # è·³è¿‡æ— æ•ˆåç¼€
                path_lower = parsed.path.lower()
                if any(path_lower.endswith(ext) for ext in invalid_extensions):
                    continue
                
                # è·³è¿‡å¹¿å‘Šé“¾æ¥
                url_lower = match.lower()
                if any(kw in url_lower for kw in ad_keywords):
                    continue
                
                # è·³è¿‡ç›¸åŒåŸŸåçš„åŸºç¡€é¡µé¢
                if parsed.netloc == base_domain and parsed.path in ['/', '', '/index.html', '/index.php']:
                    continue
                
                # è·³è¿‡æŸ¥è¯¢å‚æ•°è¿‡å¤šçš„é“¾æ¥
                if len(parsed.query) > 200:
                    continue
                
                all_links.append(match)
            
            # å»é‡
            unique_links = list(dict.fromkeys(all_links))
            
            # åˆ†ç±»é“¾æ¥
            same_domain = []
            external_domain = []
            
            for link in unique_links:
                parsed = urlparse(link)
                if parsed.netloc == base_domain:
                    same_domain.append(link)
                else:
                    external_domain.append(link)
            
            if not unique_links:
                return f"âŒ æœªåœ¨ {url} ä¸­æ‰¾åˆ°æœ‰æ•ˆé“¾æ¥"
            
            # æ ¼å¼åŒ–è¾“å‡º
            result_text = f"ğŸ”— ä» {url} æå–åˆ° {len(unique_links)} ä¸ªæœ‰æ•ˆé“¾æ¥ï¼š\n\n"
            
            if same_domain:
                result_text += f"ğŸ“‚ ç«™å†…é“¾æ¥ ({len(same_domain)} ä¸ª)ï¼š\n"
                for i, link in enumerate(same_domain[:15], 1):
                    result_text += f"  {i}. {link}\n"
                if len(same_domain) > 15:
                    result_text += f"  ... è¿˜æœ‰ {len(same_domain) - 15} ä¸ª\n"
                result_text += "\n"
            
            if external_domain:
                result_text += f"ğŸŒ å¤–éƒ¨é“¾æ¥ ({len(external_domain)} ä¸ª)ï¼š\n"
                for i, link in enumerate(external_domain[:10], 1):
                    result_text += f"  {i}. {link}\n"
                if len(external_domain) > 10:
                    result_text += f"  ... è¿˜æœ‰ {len(external_domain) - 10} ä¸ª\n"
            
            return result_text
            
        except Exception as e:
            logger.error(f"æå–é¡µé¢é“¾æ¥å¤±è´¥: {e}")
            return f"âŒ æå–é¡µé¢é“¾æ¥å¤±è´¥: {str(e)}"
    
    async def _handle_general(self, params: Dict) -> str:
        """å¤„ç†é€šç”¨è¯·æ±‚"""
        # è·å–åŸå§‹æ–‡æœ¬ï¼Œæ”¯æŒå¤šç§å‚æ•°å
        original_text = params.get("original_text", "") or params.get("text", "") or params.get("query", "") or params.get("keyword", "")
        
        if not original_text:
            return "âŒ è¯·æä¾›æœç´¢å…³é”®è¯æˆ–æ“ä½œæŒ‡ä»¤"
        
        # æ›´æ–° paramsï¼Œç¡®ä¿åç»­æ–¹æ³•å¯ä»¥è·å–åˆ° query
        if "query" not in params and "keyword" not in params:
            params["query"] = original_text
        
        if "é¡µé¢é“¾æ¥" in original_text or "æå–é“¾æ¥" in original_text:
            return await self._extract_page_links(params)
        elif "è§†é¢‘é“¾æ¥" in original_text or "mp4é“¾æ¥" in original_text.lower():
            return await self._scrape_links(params)
        elif "æœç´¢" in original_text:
            return await self._web_search(params)
        elif "æŠ“å–" in original_text or "çˆ¬å–" in original_text:
            return await self._crawl_webpage(params)
        else:
            # é»˜è®¤è¿›è¡Œç½‘ç»œæœç´¢
            return await self._web_search(params)
    
    async def _web_search(self, params: Dict) -> str:
        """ç½‘ç»œæœç´¢"""
        query = params.get("query", "") or params.get("keyword", "")
        
        if not query:
            return "âŒ è¯·æä¾›æœç´¢å…³é”®è¯"
        
        original_query = query
        
        try:
            from ...config import settings
            user_address = settings.user.address or ""
            user_city = settings.user.city or ""
            
            if user_address and user_city and user_city not in user_address:
                full_address = f"{user_city}{user_address}"
            else:
                full_address = user_address or user_city
            
            if full_address:
                location_keywords = ["å‘¨è¾¹", "é™„è¿‘", "å‘¨å›´", "å°±è¿‘", "å“ªé‡Œæœ‰", "å“ªæœ‰"]
                has_location_keyword = any(kw in query for kw in location_keywords)
                
                if has_location_keyword:
                    for kw in location_keywords:
                        query = query.replace(kw, f"{full_address}")
                    logger.info(f"ğŸ” æ›¿æ¢ä½ç½®å…³é”®è¯: {original_query} -> {query}")
        except Exception as e:
            logger.warning(f"å¤„ç†ä½ç½®ä¿¡æ¯å¤±è´¥: {e}")
        
        self._send_message_to_chat(f"ğŸ” æ­£åœ¨æœç´¢...\n\nğŸ“ å…³é”®è¯: {query}")
        
        logger.info(f"ğŸ” ç½‘ç»œæœç´¢: {query}")
        
        results = []
        
        try:
            baidu_url = f"https://www.baidu.com/s?wd={urllib.parse.quote(query)}"
            baidu_result = await self._fetch_search_results(baidu_url, "ç™¾åº¦")
            if baidu_result:
                results.extend(baidu_result)
        except Exception as e:
            logger.warning(f"ç™¾åº¦æœç´¢å¤±è´¥: {e}")
        
        if not results:
            try:
                bing_url = f"https://www.bing.com/search?q={urllib.parse.quote(query)}"
                bing_result = await self._fetch_search_results(bing_url, "å¿…åº”")
                if bing_result:
                    results.extend(bing_result)
            except Exception as e:
                logger.warning(f"å¿…åº”æœç´¢å¤±è´¥: {e}")
        
        if results:
            raw_results = []
            seen = set()
            for title, snippet, url in results[:10]:
                if title in seen:
                    continue
                seen.add(title)
                raw_results.append(f"æ ‡é¢˜: {title}\næ‘˜è¦: {snippet}\né“¾æ¥: {url}")
            
            raw_text = "\n\n".join(raw_results)
            
            try:
                llm_response = await self._summarize_search_results(query, raw_text)
                if llm_response:
                    logger.info(f"âœ… LLM æ€»ç»“æˆåŠŸï¼Œé•¿åº¦: {len(llm_response)}")
                    return llm_response
            except Exception as e:
                logger.warning(f"LLM æ€»ç»“å¤±è´¥: {e}")
            
            result_text = f"ğŸ” æœç´¢ç»“æœ: {query}\n\n"
            seen = set()
            for i, (title, snippet, url) in enumerate(results[:8], 1):
                if title in seen:
                    continue
                seen.add(title)
                result_text += f"**{i}. {title}**\n"
                if snippet:
                    result_text += f"   {snippet}\n"
                result_text += f"   ğŸ”— {url}\n\n"
            
            result_text += "ğŸ’¡ ç‚¹å‡»é“¾æ¥å¯æŸ¥çœ‹è¯¦æƒ…"
            return result_text
        else:
            search_engines = [
                ("ç™¾åº¦", f"https://www.baidu.com/s?wd={urllib.parse.quote(query)}"),
                ("å¿…åº”", f"https://www.bing.com/search?q={urllib.parse.quote(query)}"),
            ]
            result_text = f"ğŸ” æœç´¢: {query}\n\næœªèƒ½è·å–æœç´¢ç»“æœï¼Œè¯·å°è¯•ä»¥ä¸‹é“¾æ¥ï¼š\n\n"
            for name, url in search_engines:
                result_text += f"â€¢ {name}: {url}\n"
            return result_text
    
    async def _summarize_search_results(self, query: str, raw_results: str) -> Optional[str]:
        """ä½¿ç”¨ LLM æ€»ç»“æœç´¢ç»“æœ"""
        try:
            from ...llm import LLMGateway
            from ...config import settings
            
            llm = LLMGateway(settings.llm)
            
            user_address = settings.user.address or ""
            user_city = settings.user.city or ""
            
            # æ£€æŸ¥æœç´¢å…³é”®è¯ä¸­æ˜¯å¦åŒ…å«åŸå¸‚å
            # å¦‚æœç”¨æˆ·æ˜ç¡®æŒ‡å®šäº†åŸå¸‚ï¼ˆå¦‚"æ·±åœ³"ã€"åŒ—äº¬"ï¼‰ï¼Œåˆ™ä¸ä½¿ç”¨ç”¨æˆ·é»˜è®¤ä½ç½®
            common_cities = ["åŒ—äº¬", "ä¸Šæµ·", "å¹¿å·", "æ·±åœ³", "æ­å·", "æˆéƒ½", "é‡åº†", "æ­¦æ±‰", "è¥¿å®‰", "å—äº¬", "å¤©æ´¥", "è‹å·", "é•¿æ²™", "éƒ‘å·", "ä¸œè", "é’å²›", "æ²ˆé˜³", "å®æ³¢", "æ˜†æ˜"]
            has_explicit_city = any(city in query for city in common_cities)
            
            if has_explicit_city:
                # ç”¨æˆ·æ˜ç¡®æŒ‡å®šäº†åŸå¸‚ï¼Œä¸ä½¿ç”¨é»˜è®¤ä½ç½®
                location_info = ""
                logger.info(f"ğŸ“ ç”¨æˆ·æ˜ç¡®æŒ‡å®šäº†åŸå¸‚ï¼Œä¸ä½¿ç”¨é»˜è®¤ä½ç½®")
            else:
                # ç”¨æˆ·æ²¡æœ‰æŒ‡å®šåŸå¸‚ï¼Œä½¿ç”¨é»˜è®¤ä½ç½®
                if user_address and user_city and user_city not in user_address:
                    full_address = f"{user_city}{user_address}"
                else:
                    full_address = user_address or user_city or "æœªçŸ¥ä½ç½®"
                location_info = f"\nç”¨æˆ·ä½ç½®: {full_address}"
                logger.info(f"ğŸ“ ä½¿ç”¨ç”¨æˆ·é»˜è®¤ä½ç½®: {full_address}")
            
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹æœç´¢ç»“æœï¼Œä¸ºç”¨æˆ·æ•´ç†å‡ºæœ‰ç”¨çš„ä¿¡æ¯ã€‚{location_info}
ç”¨æˆ·æœç´¢: {query}

æœç´¢ç»“æœ:
{raw_results}

è¯·æŒ‰è¦æ±‚æ•´ç†:
1. æå–æœ€ç›¸å…³ã€æœ€æœ‰ä»·å€¼çš„ä¿¡æ¯
2. å¦‚æœæ˜¯æ‰¾åº—é“º/é¤å…ï¼Œåˆ—å‡ºåç§°ã€åœ°å€ã€ç‰¹è‰²ã€äººå‡æ¶ˆè´¹ç­‰ä¿¡æ¯
3. å¦‚æœæ˜¯æ‰¾æ™¯ç‚¹ï¼Œåˆ—å‡ºåç§°ã€ä½ç½®ã€ç‰¹è‰²ã€é—¨ç¥¨ç­‰ä¿¡æ¯
4. å¦‚æœæ˜¯å…¶ä»–ä¿¡æ¯ï¼Œæ•´ç†æˆæ¸…æ™°æ˜“è¯»çš„æ ¼å¼
5. å»é™¤é‡å¤å’Œæ— å…³ä¿¡æ¯
6. ç”¨ç®€æ´è‡ªç„¶çš„è¯­è¨€å›ç­”ï¼Œä¸è¦ç›´æ¥å¤åˆ¶æœç´¢ç»“æœ
7. å¦‚æœæœç´¢ç»“æœä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·

é‡è¦ï¼š
- ä¸è¦æ·»åŠ "éœ€è¦æˆ‘å¸®æ‚¨..."ä¹‹ç±»çš„åç»­æœåŠ¡å»ºè®®
- ä¸è¦æåˆ°å¯¼èˆªé“¾æ¥ã€åœ°å›¾æ ‡è®°ã€è®¾ç½®æé†’ç­‰åŠŸèƒ½
- åªè¾“å‡ºæœç´¢ç»“æœçš„æ•´ç†å†…å®¹ï¼Œä¸è¦åŠ å¼€åœºç™½å’Œç»“æŸè¯­"""

            messages = [{"role": "user", "content": prompt}]
            response = await llm.chat(messages)
            
            if response and response.content:
                return response.content
            
        except Exception as e:
            logger.error(f"LLM æ€»ç»“æœç´¢ç»“æœå¤±è´¥: {e}")
        
        return None

    async def _fetch_search_results(self, url: str, engine: str) -> List[tuple]:
        """æŠ“å–æœç´¢ç»“æœé¡µé¢"""
        results = []
        
        try:
            if PLAYWRIGHT_AVAILABLE:
                logger.info(f"ğŸŒ ä½¿ç”¨ Playwright æŠ“å–: {url}")
                async with async_playwright() as p:
                    browser = await p.chromium.launch(headless=True)
                    page = await browser.new_page()
                    
                    try:
                        await page.goto(url, wait_until='networkidle', timeout=15000)
                        await page.wait_for_timeout(2000)
                        
                        content = await page.content()
                        logger.info(f"ğŸ“„ è·å–åˆ°é¡µé¢å†…å®¹ï¼Œé•¿åº¦: {len(content)}")
                        
                        if 'baidu' in url:
                            results = self._parse_baidu_results(content)
                        elif 'bing' in url:
                            results = self._parse_bing_results(content)
                        
                        logger.info(f"âœ… è§£æåˆ° {len(results)} æ¡ç»“æœ")
                            
                    finally:
                        await browser.close()
            else:
                logger.info(f"ğŸŒ ä½¿ç”¨ aiohttp æŠ“å–: {url}")
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                }
                
                import aiohttp
                async with aiohttp.ClientSession() as session:
                    async with session.get(url, headers=headers, timeout=15) as response:
                        content = await response.text()
                        logger.info(f"ğŸ“„ è·å–åˆ°é¡µé¢å†…å®¹ï¼Œé•¿åº¦: {len(content)}")
                        
                        if 'baidu' in url:
                            results = self._parse_baidu_results(content)
                        elif 'bing' in url:
                            results = self._parse_bing_results(content)
                        
                        logger.info(f"âœ… è§£æåˆ° {len(results)} æ¡ç»“æœ")
                            
        except Exception as e:
            logger.error(f"æŠ“å–æœç´¢ç»“æœå¤±è´¥ ({engine}): {e}")
        
        return results
    
    def _parse_baidu_results(self, html: str) -> List[tuple]:
        """è§£æç™¾åº¦æœç´¢ç»“æœ"""
        results = []
        
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html, 'html.parser')
            
            selectors = [
                '.result',
                '.c-container',
                '#content_left .result',
                '.c-group-content',
                'div[class*="result"]',
            ]
            
            items = []
            for selector in selectors:
                found = soup.select(selector)
                if found:
                    items = found
                    logger.info(f"ğŸ“Š ç™¾åº¦é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(found)} ä¸ªå…ƒç´ ")
                    break
            
            if not items:
                items = soup.find_all('div', class_=lambda x: x and ('result' in x or 'container' in x))
                logger.info(f"ğŸ“Š ç™¾åº¦é€šè¿‡ class åŒ¹é…æ‰¾åˆ° {len(items)} ä¸ªå…ƒç´ ")
            
            for item in items:
                try:
                    title_elem = item.select_one('h3, .t, .c-title, a[href] em, .c-title-en')
                    title = title_elem.get_text(strip=True) if title_elem else ''
                    
                    snippet_elem = item.select_one('.c-abstract, .c-span9, .c-color-text, p[class*="content"]')
                    snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''
                    
                    link_elem = item.select_one('a[href]')
                    link = link_elem.get('href', '') if link_elem else ''
                    
                    if title and link and 'baidu.com' not in title:
                        if link.startswith('/'):
                            link = 'https://www.baidu.com' + link
                        results.append((title, snippet[:200] if snippet else '', link))
                        
                except Exception as e:
                    continue
                    
        except ImportError:
            logger.warning("BeautifulSoup æœªå®‰è£…ï¼Œä½¿ç”¨æ­£åˆ™è§£æ")
            import re
            title_pattern = r'<h3[^>]*>(.*?)</h3>'
            titles = re.findall(title_pattern, html, re.DOTALL)
            
            for title in titles[:10]:
                clean_title = re.sub(r'<[^>]+>', '', title).strip()
                if clean_title:
                    results.append((clean_title, '', ''))
        
        except Exception as e:
            logger.error(f"è§£æç™¾åº¦ç»“æœå¤±è´¥: {e}")
                    
        return results
    
    def _parse_bing_results(self, html: str) -> List[tuple]:
        """è§£æå¿…åº”æœç´¢ç»“æœ"""
        results = []
        
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html, 'html.parser')
            
            selectors = [
                '.b_algo',
                '#b_results .b_algo',
                'li.b_algo',
            ]
            
            items = []
            for selector in selectors:
                found = soup.select(selector)
                if found:
                    items = found
                    logger.info(f"ğŸ“Š å¿…åº”é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(found)} ä¸ªå…ƒç´ ")
                    break
            
            for item in items:
                try:
                    title_elem = item.select_one('h2, .b_topTitle, a[href]')
                    title = title_elem.get_text(strip=True) if title_elem else ''
                    
                    snippet_elem = item.select_one('.b_caption p, .b_paractl, p')
                    snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''
                    
                    link_elem = item.select_one('a[href]')
                    link = link_elem.get('href', '') if link_elem else ''
                    
                    if title and link:
                        results.append((title, snippet[:200] if snippet else '', link))
                        
                except Exception as e:
                    continue
                    
        except ImportError:
            logger.warning("BeautifulSoup æœªå®‰è£…ï¼Œä½¿ç”¨æ­£åˆ™è§£æ")
            import re
            title_pattern = r'<h2[^>]*>(.*?)</h2>'
            titles = re.findall(title_pattern, html, re.DOTALL)
            
            for title in titles[:10]:
                clean_title = re.sub(r'<[^>]+>', '', title).strip()
                if clean_title:
                    results.append((clean_title, '', ''))
        
        except Exception as e:
            logger.error(f"è§£æå¿…åº”ç»“æœå¤±è´¥: {e}")
            
        return results
    
    async def _scrape_links(self, params: Dict) -> str:
        """æŠ“å–ç½‘é¡µä¸­çš„é“¾æ¥"""
        url = params.get("url", "")
        link_type = params.get("link_type", "")  # video, image, all
        is_workflow = params.get("is_workflow", False)
        
        # ä» action å‚æ•°æ¨æ–­é“¾æ¥ç±»å‹
        action = params.get("action", "")
        if "m3u8" in action.lower() and not link_type:
            link_type = "m3u8"
        elif "mp4" in action.lower() and not link_type:
            link_type = "mp4"
        
        if not url:
            return "âŒ è¯·æä¾›ç½‘é¡µ URL"
        
        # æ¸…ç† URL - ç§»é™¤åå¼•å·ã€é€—å·ç­‰
        url = url.strip().strip('`').strip(',').strip()
        
        # å‘é€æç¤ºæ¶ˆæ¯
        type_desc = "é“¾æ¥" if not link_type else f"{link_type}é“¾æ¥"
        self._send_message_to_chat(f"ğŸ•·ï¸ æ­£åœ¨æŠ“å–ç½‘é¡µ{type_desc}...\n\nğŸ”— URL: {url[:80]}{'...' if len(url) > 80 else ''}")
        
        logger.info(f"ğŸ”— æŠ“å–é“¾æ¥: {url}, ç±»å‹: {link_type}, å·¥ä½œæµ: {is_workflow}")
        
        # å¯¹äºè§†é¢‘é“¾æ¥æŠ“å–ï¼Œä¼˜å…ˆä½¿ç”¨ Playwright åŠ¨æ€æŠ“å–
        if PLAYWRIGHT_AVAILABLE and ("è§†é¢‘" in str(params) or "mp4" in str(params).lower() or "m3u8" in str(params).lower() or link_type in ["mp4", "m3u8"]):
            logger.info(f"ğŸ¬ ä½¿ç”¨ Playwright åŠ¨æ€æŠ“å–è§†é¢‘é“¾æ¥...")
            dynamic_links, error = await self._scrape_dynamic_video(url)
            if dynamic_links:
                # æ ¹æ®ä»»åŠ¡ç±»å‹è¿‡æ»¤é“¾æ¥
                filtered_links = dynamic_links
                filter_type = "è§†é¢‘"
                if "mp4" in link_type.lower() or "mp4" in str(params).lower():
                    filtered_links = [l for l in dynamic_links if '.mp4' in l.lower()]
                    filter_type = "MP4è§†é¢‘"
                elif "m3u8" in link_type.lower() or "m3u8" in str(params).lower():
                    filtered_links = [l for l in dynamic_links if '.m3u8' in l.lower()]
                    filter_type = "M3U8æµåª’ä½“"
                
                if filtered_links:
                    # å¦‚æœæ˜¯å·¥ä½œæµæ¨¡å¼ï¼Œåªè¿”å›ç¬¬ä¸€ä¸ªé“¾æ¥
                    if is_workflow:
                        logger.info(f"ğŸ”„ å·¥ä½œæµæ¨¡å¼: è¿”å›ç¬¬ä¸€ä¸ªè§†é¢‘é“¾æ¥")
                        return filtered_links[0]
                    
                    result_text = f"ğŸ¬ ä» {url} æå–åˆ° {len(filtered_links)} ä¸ª{filter_type}é“¾æ¥ï¼š\n\n"
                    for i, link in enumerate(filtered_links[:20], 1):
                        result_text += f"{i}. {link}\n"
                    return result_text
        
        try:
            req = urllib.request.Request(url, headers=self._get_headers())
            
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: urllib.request.urlopen(req, timeout=60, context=self._create_ssl_context())
            )
            
            html = response.read().decode('utf-8', errors='ignore')
            
            # æå–é“¾æ¥
            links = []
            
            # è§†é¢‘é“¾æ¥æ¨¡å¼ - æ‰©å±•æ›´å¤šæ ¼å¼å’Œå±æ€§
            video_patterns = [
                r'href=["\']([^"\']+\.(?:mp4|avi|mkv|mov|wmv|flv|webm|m3u8|m3u|ts)[^"\']*)["\']',
                r'src=["\']([^"\']+\.(?:mp4|avi|mkv|mov|wmv|flv|webm|m3u8|m3u|ts)[^"\']*)["\']',
                r'data-src=["\']([^"\']+\.(?:mp4|avi|mkv|mov|wmv|flv|webm|m3u8|m3u|ts)[^"\']*)["\']',
                r'data-url=["\']([^"\']+\.(?:mp4|avi|mkv|mov|wmv|flv|webm|m3u8|m3u|ts)[^"\']*)["\']',
                r'data-video=["\']([^"\']+\.(?:mp4|avi|mkv|mov|wmv|flv|webm|m3u8|m3u|ts)[^"\']*)["\']',
                r'file:\s*["\']([^"\']+\.(?:mp4|avi|mkv|mov|wmv|flv|webm|m3u8|m3u|ts)[^"\']*)["\']',
                r'url:\s*["\']([^"\']+\.(?:mp4|avi|mkv|mov|wmv|flv|webm|m3u8|m3u|ts)[^"\']*)["\']',
                r'https?://[^\s<>"\']+\.(?:mp4|avi|mkv|mov|wmv|flv|webm|m3u8|m3u|ts)',
                r'["\']https?://[^"\']+\.(?:mp4|avi|mkv|mov|wmv|flv|webm|m3u8|m3u|ts)[^"\']*["\']',
            ]
            
            # æµåª’ä½“å’ŒAPIé“¾æ¥æ¨¡å¼
            stream_patterns = [
                r'["\']https?://[^"\']*\.m3u8[^"\']*["\']',
                r'["\']https?://[^"\']*\.ts[^"\']*["\']',
                r'["\']https?://[^"\']*/video/[^"\']*["\']',
                r'["\']https?://[^"\']*/play/[^"\']*["\']',
                r'["\']https?://[^"\']*/stream/[^"\']*["\']',
                r'["\']https?://[^"\']*/api/[^"\']*video[^"\']*["\']',
                r'videoUrl["\']?\s*[:=]\s*["\']([^"\']+)["\']',
                r'video_url["\']?\s*[:=]\s*["\']([^"\']+)["\']',
                r'playUrl["\']?\s*[:=]\s*["\']([^"\']+)["\']',
                r'play_url["\']?\s*[:=]\s*["\']([^"\']+)["\']',
                r'source["\']?\s*[:=]\s*["\']([^"\']+)["\']',
            ]
            
            # å›¾ç‰‡é“¾æ¥æ¨¡å¼
            image_patterns = [
                r'src=["\']([^"\']+\.(?:jpg|jpeg|png|gif|webp|bmp)[^"\']*)["\']',
                r'href=["\']([^"\']+\.(?:jpg|jpeg|png|gif|webp|bmp)[^"\']*)["\']',
                r'data-src=["\']([^"\']+\.(?:jpg|jpeg|png|gif|webp|bmp)[^"\']*)["\']',
            ]
            
            # æ‰€æœ‰é“¾æ¥æ¨¡å¼
            all_patterns = [
                r'href=["\']([^"\']+)["\']',
                r'src=["\']([^"\']+)["\']',
            ]
            
            if link_type == "video" or "è§†é¢‘" in str(params):
                patterns = video_patterns + stream_patterns
                type_name = "è§†é¢‘"
            elif link_type == "image" or "å›¾ç‰‡" in str(params):
                patterns = image_patterns
                type_name = "å›¾ç‰‡"
            else:
                patterns = video_patterns + stream_patterns + all_patterns
                type_name = "è§†é¢‘"
            
            for pattern in patterns:
                matches = re.findall(pattern, html, re.IGNORECASE)
                for match in matches:
                    if isinstance(match, tuple):
                        match = match[0] if match[0] else match[1] if len(match) > 1 else ""
                    if match:
                        # æ¸…ç†å¼•å·
                        match = match.strip('"\'')
                        if match and match not in links:
                            # è¿‡æ»¤æ‰æ— æ•ˆé“¾æ¥
                            if match.startswith('http') or match.startswith('/'):
                                if not match.startswith('http'):
                                    # è¡¥å…¨ç›¸å¯¹è·¯å¾„
                                    from urllib.parse import urljoin
                                    match = urljoin(url, match)
                                # è¿‡æ»¤æ‰ä¸€äº›æ˜æ˜¾ä¸æ˜¯è§†é¢‘çš„é“¾æ¥
                                if not any(x in match.lower() for x in ['.css', '.js', '.json', '.woff', '.ttf', '.ico']):
                                    links.append(match)
            
            if not links:
                # å¦‚æœé™æ€æŠ“å–å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ Playwright åŠ¨æ€æŠ“å–
                if PLAYWRIGHT_AVAILABLE:
                    logger.info(f"é™æ€æŠ“å–æœªæ‰¾åˆ°é“¾æ¥ï¼Œå°è¯• Playwright åŠ¨æ€æŠ“å–...")
                    dynamic_links, error = await self._scrape_dynamic_video(url)
                    if dynamic_links:
                        # æ ¹æ®ä»»åŠ¡ç±»å‹è¿‡æ»¤é“¾æ¥
                        filtered_links = dynamic_links
                        filter_type = type_name
                        if "mp4" in link_type.lower() or "mp4" in str(params).lower():
                            filtered_links = [l for l in dynamic_links if '.mp4' in l.lower()]
                            filter_type = "MP4è§†é¢‘"
                        elif "m3u8" in link_type.lower() or "m3u8" in str(params).lower():
                            filtered_links = [l for l in dynamic_links if '.m3u8' in l.lower()]
                            filter_type = "M3U8æµåª’ä½“"
                        
                        if filtered_links:
                            result_text = f"ğŸ¬ ä» {url} æå–åˆ° {len(filtered_links)} ä¸ª{filter_type}é“¾æ¥ï¼ˆåŠ¨æ€æŠ“å–ï¼‰ï¼š\n\n"
                            for i, link in enumerate(filtered_links[:20], 1):
                                result_text += f"{i}. {link}\n"
                            return result_text
                
                # å¦‚æœåŠ¨æ€æŠ“å–ä¹Ÿå¤±è´¥ï¼Œè¿”å›é¡µé¢ä¸­çš„æ‰€æœ‰URLä¾›å‚è€ƒ
                all_urls = re.findall(r'https?://[^\s<>"\']+', html)
                all_urls = [u.strip('"\'') for u in all_urls if not any(x in u.lower() for x in ['.css', '.js', '.json', '.woff', '.ttf', '.ico', '.png', '.jpg', '.gif'])]
                unique_urls = list(dict.fromkeys(all_urls))[:10]
                
                if unique_urls:
                    result_text = f"âš ï¸ æœªæ‰¾åˆ°ç›´æ¥çš„è§†é¢‘é“¾æ¥ï¼Œä½†å‘ç°ä»¥ä¸‹URLå¯èƒ½åŒ…å«è§†é¢‘èµ„æºï¼š\n\n"
                    for i, link in enumerate(unique_urls, 1):
                        result_text += f"{i}. {link}\n"
                    result_text += f"\nğŸ’¡ æç¤ºï¼šè¯¥ç½‘ç«™å¯èƒ½ä½¿ç”¨JavaScriptåŠ¨æ€åŠ è½½è§†é¢‘ï¼Œå»ºè®®åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹ç½‘ç»œè¯·æ±‚è·å–çœŸå®è§†é¢‘åœ°å€ã€‚"
                    return result_text
                else:
                    return f"âŒ æœªåœ¨ {url} ä¸­æ‰¾åˆ°è§†é¢‘é“¾æ¥\n\nğŸ’¡ æç¤ºï¼šè¯¥ç½‘ç«™å¯èƒ½ä½¿ç”¨JavaScriptåŠ¨æ€åŠ è½½è§†é¢‘å†…å®¹ï¼Œå»ºè®®ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·æŸ¥çœ‹ç½‘ç»œè¯·æ±‚ã€‚"
            
            # å»é‡å¹¶é™åˆ¶æ•°é‡
            unique_links = list(dict.fromkeys(links))[:20]
            
            # æ ¹æ®ä»»åŠ¡ç±»å‹è¿‡æ»¤é“¾æ¥
            if "mp4" in link_type.lower() or "mp4" in str(params).lower():
                unique_links = [l for l in unique_links if '.mp4' in l.lower()]
                type_name = "MP4è§†é¢‘"
            elif "m3u8" in link_type.lower() or "m3u8" in str(params).lower():
                unique_links = [l for l in unique_links if '.m3u8' in l.lower()]
                type_name = "M3U8æµåª’ä½“"
            
            if not unique_links:
                return f"âŒ æœªåœ¨ {url} ä¸­æ‰¾åˆ°{type_name}é“¾æ¥"
            
            # å¦‚æœæ˜¯å·¥ä½œæµæ¨¡å¼ï¼Œåªè¿”å›ç¬¬ä¸€ä¸ªé“¾æ¥
            if is_workflow and unique_links:
                logger.info(f"ğŸ”„ å·¥ä½œæµæ¨¡å¼: è¿”å›ç¬¬ä¸€ä¸ªè§†é¢‘é“¾æ¥")
                return unique_links[0]
            
            result_text = f"ğŸ”— ä» {url} æå–åˆ° {len(unique_links)} ä¸ª{type_name}é“¾æ¥ï¼š\n\n"
            for i, link in enumerate(unique_links, 1):
                result_text += f"{i}. {link}\n"
            
            return result_text
            
        except Exception as e:
            logger.error(f"é™æ€æŠ“å–é“¾æ¥å¤±è´¥: {e}")
            
            # é™æ€æŠ“å–å¤±è´¥æ—¶ï¼Œå°è¯• Playwright åŠ¨æ€æŠ“å–
            if PLAYWRIGHT_AVAILABLE:
                logger.info(f"é™æ€æŠ“å–è¶…æ—¶ï¼Œå°è¯• Playwright åŠ¨æ€æŠ“å–...")
                try:
                    dynamic_links, error = await self._scrape_dynamic_video(url)
                    if dynamic_links:
                        # æ ¹æ®ä»»åŠ¡ç±»å‹è¿‡æ»¤é“¾æ¥
                        filtered_links = dynamic_links
                        filter_type = "è§†é¢‘"
                        if "mp4" in link_type.lower() or "mp4" in str(params).lower():
                            filtered_links = [l for l in dynamic_links if '.mp4' in l.lower()]
                            filter_type = "MP4è§†é¢‘"
                        elif "m3u8" in link_type.lower() or "m3u8" in str(params).lower():
                            filtered_links = [l for l in dynamic_links if '.m3u8' in l.lower()]
                            filter_type = "M3U8æµåª’ä½“"
                        
                        if filtered_links:
                            # å¦‚æœæ˜¯å·¥ä½œæµæ¨¡å¼ï¼Œåªè¿”å›ç¬¬ä¸€ä¸ªé“¾æ¥
                            if is_workflow:
                                logger.info(f"ğŸ”„ å·¥ä½œæµæ¨¡å¼: è¿”å›ç¬¬ä¸€ä¸ªè§†é¢‘é“¾æ¥")
                                return filtered_links[0]
                            
                            result_text = f"ğŸ¬ ä» {url} æå–åˆ° {len(filtered_links)} ä¸ª{filter_type}é“¾æ¥ï¼ˆåŠ¨æ€æŠ“å–ï¼‰ï¼š\n\n"
                            for i, link in enumerate(filtered_links[:20], 1):
                                result_text += f"{i}. {link}\n"
                            return result_text
                except Exception as e2:
                    logger.error(f"åŠ¨æ€æŠ“å–ä¹Ÿå¤±è´¥: {e2}")
            
            return f"âŒ æŠ“å–é“¾æ¥å¤±è´¥: {str(e)}"
    
    async def _scrape_dynamic_video(self, url: str) -> Tuple[List[str], str]:
        """ä½¿ç”¨ Playwright æŠ“å–åŠ¨æ€åŠ è½½çš„è§†é¢‘é“¾æ¥"""
        if not PLAYWRIGHT_AVAILABLE:
            return [], "Playwright æœªå®‰è£…ï¼Œæ— æ³•æŠ“å–åŠ¨æ€å†…å®¹"
        
        video_links = []
        m3u8_links = []
        ts_links = []
        error_msg = ""
        
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context(
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                )
                page = await context.new_page()
                
                # ç›‘å¬ç½‘ç»œè¯·æ±‚
                async def handle_response(response):
                    url_str = response.url
                    # æ£€æŸ¥æ˜¯å¦æ˜¯è§†é¢‘ç›¸å…³è¯·æ±‚
                    if '.m3u8' in url_str.lower():
                        logger.info(f"ğŸ¬ å‘ç° m3u8 è¯·æ±‚: {url_str[:100]}...")
                        m3u8_links.append(url_str)
                    elif '.ts' in url_str.lower() and '.m3u8' not in url_str.lower():
                        logger.info(f"ğŸ¬ å‘ç° TS åˆ†ç‰‡: {url_str[:80]}...")
                        ts_links.append(url_str)
                    elif any(ext in url_str.lower() for ext in ['.mp4', '.flv', '.webm', '.mkv', '.avi']):
                        logger.info(f"ğŸ¬ å‘ç°è§†é¢‘è¯·æ±‚: {url_str[:100]}...")
                        video_links.append(url_str)
                    # æ£€æŸ¥æ˜¯å¦æ˜¯è§†é¢‘APIè¯·æ±‚
                    elif any(keyword in url_str.lower() for keyword in ['/video/', '/play/', '/stream/', 'playurl', 'videourl', 'video_url', 'getvideo']):
                        if not any(ext in url_str.lower() for ext in ['.css', '.js', '.png', '.jpg', '.gif', '.ico', '.woff', '.ttf']):
                            logger.info(f"ğŸ¬ å‘ç°è§†é¢‘APIè¯·æ±‚: {url_str}")
                            video_links.append(url_str)
                
                page.on("response", handle_response)
                
                # è®¿é—®é¡µé¢
                logger.info(f"ğŸ­ Playwright æ­£åœ¨è®¿é—®: {url}")
                await page.goto(url, wait_until="domcontentloaded", timeout=60000)
                
                # ç­‰å¾…é¡µé¢åŠ è½½
                await page.wait_for_timeout(3000)
                
                # å°è¯•ç‚¹å‡»æ’­æ”¾æŒ‰é’®
                play_selectors = [
                    'button[class*="play"]',
                    '.play-button',
                    '.vjs-big-play-button',
                    '[class*="play-btn"]',
                    'video',
                    '.player',
                    '[class*="player"]',
                ]
                for selector in play_selectors:
                    try:
                        element = await page.query_selector(selector)
                        if element:
                            await element.click(timeout=2000)
                            logger.info(f"ğŸ­ ç‚¹å‡»äº†æ’­æ”¾æŒ‰é’®: {selector}")
                            break
                    except:
                        pass
                
                # ç­‰å¾…è§†é¢‘å¼€å§‹æ’­æ”¾ï¼Œæ•è·æ›´å¤š TS åˆ†ç‰‡
                logger.info("ğŸ­ ç­‰å¾…è§†é¢‘æ’­æ”¾ï¼Œæ•è·åˆ†ç‰‡...")
                await page.wait_for_timeout(5000)
                
                # å°è¯•æ‹–åŠ¨è¿›åº¦æ¡åˆ°ä¸åŒä½ç½®ï¼Œè§¦å‘åŠ è½½æ›´å¤šåˆ†ç‰‡
                try:
                    video = await page.query_selector('video')
                    if video:
                        # è·å–è§†é¢‘æ—¶é•¿
                        duration = await video.evaluate('el => el.duration || 0')
                        logger.info(f"ğŸ­ è§†é¢‘æ—¶é•¿: {duration:.1f} ç§’")
                        
                        # è·³è½¬åˆ°ä¸åŒä½ç½®è§¦å‘åŠ è½½
                        positions = [0.1, 0.3, 0.5, 0.7, 0.9]
                        for pos in positions:
                            try:
                                await video.evaluate(f'el => el.currentTime = el.duration * {pos}')
                                await page.wait_for_timeout(2000)
                            except:
                                pass
                except Exception as e:
                    logger.warning(f"æ‹–åŠ¨è¿›åº¦æ¡å¤±è´¥: {e}")
                
                # æ»šåŠ¨é¡µé¢è§¦å‘æ‡’åŠ è½½
                await page.evaluate("window.scrollBy(0, 500)")
                await page.wait_for_timeout(3000)
                
                # å°è¯•æ‰§è¡Œ JavaScript è·å–æ’­æ”¾å™¨é…ç½®
                try:
                    player_info = await page.evaluate('''() => {
                        const result = {};
                        // å°è¯•è·å–æ’­æ”¾å™¨å®ä¾‹
                        if (window.__playinfo__) result.playinfo = window.__playinfo__;
                        if (window.player) result.player = window.player;
                        // å°è¯•è·å– video å…ƒç´ ä¿¡æ¯
                        const videos = document.querySelectorAll('video');
                        if (videos.length > 0) {
                            result.videoSrc = videos[0].src;
                            result.videoCurrentSrc = videos[0].currentSrc;
                        }
                        return result;
                    }''')
                    if player_info:
                        logger.info(f"ğŸ­ æ’­æ”¾å™¨ä¿¡æ¯: {str(player_info)[:200]}")
                        # ä»æ’­æ”¾å™¨ä¿¡æ¯ä¸­æå–é“¾æ¥
                        if player_info.get('videoSrc'):
                            video_links.append(player_info['videoSrc'])
                        if player_info.get('videoCurrentSrc'):
                            video_links.append(player_info['videoCurrentSrc'])
                except Exception as e:
                    logger.warning(f"è·å–æ’­æ”¾å™¨ä¿¡æ¯å¤±è´¥: {e}")
                
                # å¦‚æœè¿˜æ²¡æœ‰æ•è·åˆ°è¶³å¤Ÿçš„åˆ†ç‰‡ï¼Œç»§ç»­ç­‰å¾…
                if len(ts_links) < 20 and not m3u8_links:
                    logger.info("ğŸ­ ç»§ç»­ç­‰å¾…æ•è·æ›´å¤šåˆ†ç‰‡...")
                    await page.wait_for_timeout(10000)
                
                # ä»é¡µé¢æºç ä¸­æå–
                content = await page.content()
                
                # æå– m3u8 é“¾æ¥
                m3u8_pattern = r'https?://[^\s<>"\']+[^\s<>"\']*\.m3u8[^\s<>"\']*'
                m3u8_matches = re.findall(m3u8_pattern, content, re.IGNORECASE)
                for m in m3u8_matches:
                    if m not in m3u8_links:
                        m3u8_links.append(m)
                
                # æå– mp4 é“¾æ¥
                mp4_pattern = r'https?://[^\s<>"\']+[^\s<>"\']*\.mp4[^\s<>"\']*'
                mp4_matches = re.findall(mp4_pattern, content, re.IGNORECASE)
                for m in mp4_matches:
                    if m not in video_links:
                        video_links.append(m)
                
                # æå– JSON ä¸­çš„è§†é¢‘é“¾æ¥
                json_patterns = [
                    r'"url"\s*:\s*"([^"]+\.(?:mp4|m3u8)[^"]*)"',
                    r'"src"\s*:\s*"([^"]+\.(?:mp4|m3u8)[^"]*)"',
                    r'"source"\s*:\s*"([^"]+\.(?:mp4|m3u8)[^"]*)"',
                    r'"videoUrl"\s*:\s*"([^"]+)"',
                    r'"playUrl"\s*:\s*"([^"]+)"',
                ]
                for pattern in json_patterns:
                    matches = re.findall(pattern, content, re.IGNORECASE)
                    for m in matches:
                        if m.startswith('http'):
                            if '.m3u8' in m.lower():
                                if m not in m3u8_links:
                                    m3u8_links.append(m)
                            elif m not in video_links:
                                video_links.append(m)
                
                await browser.close()
                
                logger.info(f"ğŸ­ Playwright æŠ“å–å®Œæˆ: m3u8={len(m3u8_links)}, ts={len(ts_links)}, video={len(video_links)}")
                
        except Exception as e:
            error_msg = str(e)
            logger.error(f"Playwright æŠ“å–å¤±è´¥: {e}")
        
        # å¦‚æœæ²¡æœ‰æ•è·åˆ° m3u8ï¼Œå°è¯•ä» TS åˆ†ç‰‡ URL æ¨æ–­ m3u8 åœ°å€
        if not m3u8_links and ts_links:
            for ts_url in ts_links[:3]:
                # è…¾è®¯è§†é¢‘ TS URL æ ¼å¼: https://xxx.gtimg.com/KEY/xxx
                # å°è¯•æå–åŸºç¡€è·¯å¾„
                m3u8_match = re.search(r'(https?://[^/]+/[A-Za-z0-9_-]+/)', ts_url)
                if m3u8_match:
                    base_url = m3u8_match.group(1)
                    # å°è¯•å¤šç§å¯èƒ½çš„ m3u8 è·¯å¾„
                    potential_m3u8_list = [
                        base_url + 'index.m3u8',
                        base_url + 'playlist.m3u8',
                        base_url.rstrip('/') + '.m3u8',
                    ]
                    for potential_m3u8 in potential_m3u8_list:
                        logger.info(f"ğŸ­ å°è¯•æ¨æ–­çš„ m3u8 åœ°å€: {potential_m3u8}")
                        m3u8_links.append(potential_m3u8)
                    break
        
        # åˆå¹¶å¹¶å»é‡ï¼Œä¼˜å…ˆ m3u8 > ts > video
        all_links = list(dict.fromkeys(m3u8_links + ts_links + video_links))
        return all_links, error_msg
    
    async def _extract_links_with_playwright(self, url: str, ad_keywords: list, invalid_extensions: list, invalid_patterns: list) -> List[str]:
        """ä½¿ç”¨ Playwright æå–é¡µé¢é“¾æ¥"""
        if not PLAYWRIGHT_AVAILABLE:
            return []
        
        all_links = []
        
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context(
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                )
                page = await context.new_page()
                
                # ç›‘å¬ç½‘ç»œè¯·æ±‚
                async def handle_response(response):
                    url_str = response.url
                    # è¿‡æ»¤å¹¿å‘Šå’Œæ— æ•ˆé“¾æ¥
                    url_lower = url_str.lower()
                    if any(kw in url_lower for kw in ad_keywords):
                        return
                    if any(url_str.lower().endswith(ext) for ext in invalid_extensions):
                        return
                    all_links.append(url_str)
                
                page.on("response", handle_response)
                
                # è®¿é—®é¡µé¢
                logger.info(f"ğŸ­ Playwright æ­£åœ¨è®¿é—®: {url}")
                await page.goto(url, wait_until="domcontentloaded", timeout=60000)
                
                # ç­‰å¾…é¡µé¢åŠ è½½
                await page.wait_for_timeout(3000)
                
                # æ»šåŠ¨é¡µé¢è§¦å‘æ‡’åŠ è½½
                for i in range(3):
                    await page.evaluate("window.scrollBy(0, 500)")
                    await page.wait_for_timeout(1000)
                
                # ä»é¡µé¢æºç ä¸­æå–é“¾æ¥
                content = await page.content()
                
                from urllib.parse import urljoin, urlparse
                base_domain = urlparse(url).netloc
                
                # æå– href é“¾æ¥
                href_pattern = r'href=["\']([^"\']+)["\']'
                href_matches = re.findall(href_pattern, content, re.IGNORECASE)
                
                for match in href_matches:
                    match = match.strip()
                    
                    # è·³è¿‡æ— æ•ˆæ¨¡å¼
                    if any(re.match(p, match, re.IGNORECASE) for p in invalid_patterns):
                        continue
                    
                    # è¡¥å…¨ç›¸å¯¹è·¯å¾„
                    if not match.startswith('http'):
                        match = urljoin(url, match)
                    
                    # è¿‡æ»¤å¹¿å‘Šå’Œæ— æ•ˆé“¾æ¥
                    url_lower = match.lower()
                    if any(kw in url_lower for kw in ad_keywords):
                        continue
                    
                    try:
                        parsed = urlparse(match)
                        path_lower = parsed.path.lower()
                        if any(path_lower.endswith(ext) for ext in invalid_extensions):
                            continue
                    except:
                        continue
                    
                    all_links.append(match)
                
                await browser.close()
                
                logger.info(f"ğŸ­ Playwright æå–é“¾æ¥å®Œæˆ: {len(all_links)} ä¸ª")
                
        except Exception as e:
            logger.error(f"Playwright æå–é“¾æ¥å¤±è´¥: {e}")
        
        # å»é‡
        return list(dict.fromkeys(all_links))
    
    async def _scrape_links_with_playwright(self, params: Dict) -> str:
        """ä½¿ç”¨ Playwright æŠ“å–åŠ¨æ€é¡µé¢çš„è§†é¢‘é“¾æ¥"""
        url = params.get("url", "")
        
        if not url:
            return "âŒ è¯·æä¾›ç½‘é¡µ URL"
        
        if not PLAYWRIGHT_AVAILABLE:
            return "âŒ Playwright æœªå®‰è£…ï¼Œæ— æ³•æŠ“å–åŠ¨æ€é¡µé¢ã€‚\n\nè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š\npip install playwright\nplaywright install chromium"
        
        logger.info(f"ğŸ­ ä½¿ç”¨ Playwright æŠ“å–åŠ¨æ€é¡µé¢: {url}")
        
        video_links, error = await self._scrape_dynamic_video(url)
        
        if video_links:
            result_text = f"ğŸ¬ ä» {url} æå–åˆ° {len(video_links)} ä¸ªè§†é¢‘é“¾æ¥ï¼š\n\n"
            for i, link in enumerate(video_links[:20], 1):
                result_text += f"{i}. {link}\n"
            return result_text
        else:
            return f"âŒ æœªæ‰¾åˆ°è§†é¢‘é“¾æ¥ã€‚\n\né”™è¯¯ä¿¡æ¯: {error}\n\nğŸ’¡ æç¤ºï¼šè¯¥ç½‘ç«™å¯èƒ½æœ‰é˜²çˆ¬æœºåˆ¶æˆ–éœ€è¦ç™»å½•ã€‚"

    async def _search_mp3(self, params: Dict) -> str:
        """
        æœç´¢ MP3 éŸ³ä¹é“¾æ¥
        ä½¿ç”¨å›½å†…å¯ç”¨çš„éŸ³ä¹æºå¹¶è§£æçœŸå® MP3 é“¾æ¥
        """
        keyword = params.get("keyword", "")
        artist = params.get("artist", "")

        if not keyword:
            return "âŒ è¯·æä¾›æœç´¢å…³é”®è¯"

        search_query = f"{artist} {keyword}" if artist else keyword
        logger.info(f"ğŸµ æœç´¢ MP3: {search_query}")

        # åˆ›å»ºä»»åŠ¡
        task_id = f"mp3_{datetime.now().strftime('%Y%m%d%H%M%S')}"
        crawl_task = CrawlTask(
            task_id=task_id,
            keyword=search_query,
            task_type="mp3"
        )
        self.tasks[task_id] = crawl_task
        self.active_tasks.add(task_id)

        try:
            crawl_task.status = "running"

            # ä½¿ç”¨å¤šä¸ªè§£ææºè·å–çœŸå® MP3 é“¾æ¥
            all_results = []

            # 1. ä½¿ç”¨ç¬¬ä¸‰æ–¹è§£æ APIï¼ˆä¼˜å…ˆï¼‰
            api_results = await self._search_from_music_api(search_query)
            all_results.extend(api_results)

            # 2. ä½¿ç”¨ç½‘é¡µæŠ“å–è§£æ
            if len(all_results) < 3:
                web_results = await self._search_from_music_websites(search_query)
                all_results.extend(web_results)

            # å»é‡å¹¶æ’åº
            seen_urls = set()
            unique_results = []
            for r in all_results:
                if r.url not in seen_urls and r.url.endswith(('.mp3', '.m4a', '.flac')):
                    seen_urls.add(r.url)
                    unique_results.append(r)

            crawl_task.results = unique_results[:10]  # æœ€å¤šä¿ç•™10ä¸ªç»“æœ
            crawl_task.status = "completed"
            crawl_task.completed_at = datetime.now()

            # æ ¼å¼åŒ–ç»“æœ
            if crawl_task.results:
                result_text = f"ğŸµ æ‰¾åˆ° {len(crawl_task.results)} ä¸ª MP3 èµ„æº:\n\n"
                for i, result in enumerate(crawl_task.results[:5], 1):
                    result_text += f"{i}. {result.title}\n"
                    result_text += f"   é“¾æ¥: {result.url[:80]}...\n"
                    result_text += f"   æ¥æº: {result.source}\n"
                    if result.quality != "unknown":
                        result_text += f"   éŸ³è´¨: {result.quality}\n"
                    result_text += "\n"

                # ä¿å­˜åˆ°æ–‡ä»¶
                await self._save_results_to_file(task_id, crawl_task.results)
                result_text += f"ğŸ“ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: data/crawler/{task_id}.json"
            else:
                result_text = f"âŒ æœªæ‰¾åˆ° MP3 èµ„æº: {search_query}\n\nå»ºè®®:\n1. å°è¯•ä½¿ç”¨è‹±æ–‡å…³é”®è¯\n2. æ£€æŸ¥æ­Œæ›²åæ˜¯å¦æ­£ç¡®\n3. å°è¯•æ·»åŠ æ­Œæ‰‹å"

            self.active_tasks.discard(task_id)
            return result_text

        except Exception as e:
            crawl_task.status = "failed"
            crawl_task.error_message = str(e)
            crawl_task.completed_at = datetime.now()
            self.active_tasks.discard(task_id)
            logger.error(f"MP3 æœç´¢å¤±è´¥: {e}")
            return f"âŒ MP3 æœç´¢å¤±è´¥: {str(e)}"

    async def _search_from_music_api(self, keyword: str) -> List[SearchResult]:
        """ä½¿ç”¨éŸ³ä¹ API æœç´¢çœŸå® MP3 é“¾æ¥"""
        results = []

        # ä½¿ç”¨å¤šä¸ªå…è´¹éŸ³ä¹ API
        api_tasks = [
            self._api_liumingye(keyword),
            self._api_wyymusic(keyword),
            self._api_kuwo(keyword),
        ]

        api_results = await asyncio.gather(*api_tasks, return_exceptions=True)

        for result in api_results:
            if isinstance(result, list):
                results.extend(result)
            elif isinstance(result, Exception):
                logger.warning(f"API æœç´¢å¤±è´¥: {result}")

        return results

    async def _api_liumingye(self, keyword: str) -> List[SearchResult]:
        """ä½¿ç”¨ liumingye.cn API æœç´¢"""
        results = []
        try:
            # è¿™ä¸ª API æä¾›çœŸå®çš„ MP3 é“¾æ¥
            search_url = f"https://api.liumingye.cn/m/api/search?keyword={urllib.parse.quote(keyword)}&page=1"

            headers = self._get_headers("https://www.liumingye.cn")
            headers['Accept'] = 'application/json'

            req = urllib.request.Request(search_url, headers=headers)

            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: urllib.request.urlopen(req, timeout=15, context=self._create_ssl_context())
            )

            data = json.loads(response.read().decode('utf-8'))

            if data.get('code') == 200 and data.get('data', {}).get('list'):
                for item in data['data']['list'][:5]:
                    song_id = item.get('id')
                    if song_id:
                        # è·å–æ’­æ”¾é“¾æ¥
                        play_url = f"https://api.liumingye.cn/m/api/link?id={song_id}"
                        play_req = urllib.request.Request(play_url, headers=headers)
                        play_response = await loop.run_in_executor(
                            None,
                            lambda: urllib.request.urlopen(play_req, timeout=15, context=self._create_ssl_context())
                        )
                        play_data = json.loads(play_response.read().decode('utf-8'))

                        if play_data.get('code') == 200 and play_data.get('data'):
                            mp3_url = play_data['data'].get('url')
                            if mp3_url:
                                results.append(SearchResult(
                                    title=f"{item.get('name', 'Unknown')} - {item.get('artist', 'Unknown')}",
                                    url=mp3_url,
                                    source="liumingye.cn",
                                    quality=item.get('quality', '128kbps'),
                                    extra_info={
                                        "song_id": song_id,
                                        "album": item.get('album', ''),
                                        "duration": item.get('duration', 0)
                                    }
                                ))

            logger.info(f"liumingye.cn æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            return results

        except Exception as e:
            logger.warning(f"liumingye.cn API å¤±è´¥: {e}")
            return []

    async def _api_wyymusic(self, keyword: str) -> List[SearchResult]:
        """ä½¿ç”¨ç½‘æ˜“äº‘éŸ³ä¹ API æœç´¢"""
        results = []
        try:
            # ä½¿ç”¨å¤šä¸ªå¯ç”¨çš„ç½‘æ˜“äº‘ API
            apis = [
                ("https://api.injahow.cn/meting/?type=search&id=netease&", "search", "title"),
                ("https://music.api.lolimi.cn/?type=search&id=netease&", "search", "title"),
            ]

            headers = self._get_headers()
            headers['Accept'] = 'application/json'

            loop = asyncio.get_event_loop()

            for api_base, search_param, title_field in apis:
                try:
                    search_url = f"{api_base}{search_param}={urllib.parse.quote(keyword)}"

                    req = urllib.request.Request(search_url, headers=headers)
                    response = await loop.run_in_executor(
                        None,
                        lambda: urllib.request.urlopen(req, timeout=10, context=self._create_ssl_context())
                    )

                    data = json.loads(response.read().decode('utf-8', errors='ignore'))

                    if isinstance(data, list) and len(data) > 0:
                        for song in data[:5]:
                            song_id = song.get('song_id') or song.get('id')
                            if song_id:
                                # è·å–æ’­æ”¾é“¾æ¥
                                url_endpoint = f"{api_base}type=url&id={song_id}"
                                url_req = urllib.request.Request(url_endpoint, headers=headers)
                                url_response = await loop.run_in_executor(
                                    None,
                                    lambda: urllib.request.urlopen(url_req, timeout=10, context=self._create_ssl_context())
                                )
                                url_data = json.loads(url_response.read().decode('utf-8', errors='ignore'))

                                mp3_url = url_data.get('url') if isinstance(url_data, dict) else None
                                if mp3_url:
                                    results.append(SearchResult(
                                        title=f"{song.get('name', song.get('title', 'Unknown'))} - {song.get('artist', song.get('author', 'Unknown'))}",
                                        url=mp3_url,
                                        source="ç½‘æ˜“äº‘éŸ³ä¹",
                                        quality="128kbps",
                                        extra_info={
                                            "song_id": song_id,
                                            "album": song.get('album', ''),
                                            "pic": song.get('pic', song.get('cover', ''))
                                        }
                                    ))

                        if results:  # å¦‚æœæ‰¾åˆ°ç»“æœï¼Œè·³å‡ºå¾ªç¯
                            break

                except Exception as api_e:
                    logger.warning(f"API {api_base} å¤±è´¥: {api_e}")
                    continue

            logger.info(f"ç½‘æ˜“äº‘éŸ³ä¹ API æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            return results

        except Exception as e:
            logger.warning(f"ç½‘æ˜“äº‘éŸ³ä¹ API å¤±è´¥: {e}")
            return []

    async def _api_kuwo(self, keyword: str) -> List[SearchResult]:
        """ä½¿ç”¨é…·æˆ‘éŸ³ä¹ API æœç´¢"""
        results = []
        try:
            import gzip

            # é…·æˆ‘éŸ³ä¹æœç´¢ API
            search_url = f"http://search.kuwo.cn/r.s?all={urllib.parse.quote(keyword)}&ft=music&itemset=web_2013&client=kt&pn=0&rn=5&rformat=json&encoding=utf8"

            headers = self._get_headers()
            headers['Accept'] = 'application/json'
            headers['Accept-Encoding'] = 'gzip, deflate'

            req = urllib.request.Request(search_url, headers=headers)

            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: urllib.request.urlopen(req, timeout=15, context=self._create_ssl_context())
            )

            # å¤„ç† gzip å‹ç¼©
            content = response.read()
            try:
                # å°è¯•è§£å‹ gzip
                content = gzip.decompress(content)
            except:
                pass  # å¦‚æœä¸æ˜¯ gzipï¼Œä¿æŒåŸæ ·

            data = json.loads(content.decode('utf-8', errors='ignore'))

            if data.get('abslist'):
                for item in data['abslist'][:5]:
                    music_id = item.get('MUSICRID', '').replace('MUSIC_', '')
                    if music_id:
                        # è·å–æ’­æ”¾é“¾æ¥
                        mp3_url = f"http://antiserver.kuwo.cn/anti.s?type=convert_url&rid={music_id}&format=mp3&response=url"

                        mp3_req = urllib.request.Request(mp3_url, headers=headers)
                        mp3_response = await loop.run_in_executor(
                            None,
                            lambda: urllib.request.urlopen(mp3_req, timeout=15, context=self._create_ssl_context())
                        )

                        real_url = mp3_response.read().decode('utf-8', errors='ignore').strip()
                        if real_url and real_url.startswith('http'):
                            results.append(SearchResult(
                                title=f"{item.get('SONGNAME', 'Unknown')} - {item.get('ARTIST', 'Unknown')}",
                                url=real_url,
                                source="é…·æˆ‘éŸ³ä¹",
                                quality="128kbps",
                                extra_info={
                                    "music_id": music_id,
                                    "album": item.get('ALBUM', ''),
                                    "duration": item.get('DURATION', 0)
                                }
                            ))

            logger.info(f"é…·æˆ‘éŸ³ä¹ API æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            return results

        except Exception as e:
            logger.warning(f"é…·æˆ‘éŸ³ä¹ API å¤±è´¥: {e}")
            return []

    async def _search_from_music_websites(self, keyword: str) -> List[SearchResult]:
        """ä»éŸ³ä¹ç½‘ç«™æŠ“å– MP3 é“¾æ¥"""
        results = []

        # å°è¯•ä»å…è´¹éŸ³ä¹ç½‘ç«™æŠ“å–
        web_tasks = [
            self._crawl_music_123(keyword),
            self._crawl_music_juice(keyword),
        ]

        web_results = await asyncio.gather(*web_tasks, return_exceptions=True)

        for result in web_results:
            if isinstance(result, list):
                results.extend(result)
            elif isinstance(result, Exception):
                logger.warning(f"ç½‘é¡µæŠ“å–å¤±è´¥: {result}")

        return results

    async def _crawl_music_123(self, keyword: str) -> List[SearchResult]:
        """ä» music.123 ç±»ç½‘ç«™æŠ“å–"""
        # è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œå®é™…å®ç°éœ€è¦æ ¹æ®å…·ä½“ç½‘ç«™ç»“æ„è°ƒæ•´
        return []

    async def _crawl_music_juice(self, keyword: str) -> List[SearchResult]:
        """ä» music juice ç±»ç½‘ç«™æŠ“å–"""
        # è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œå®é™…å®ç°éœ€è¦æ ¹æ®å…·ä½“ç½‘ç«™ç»“æ„è°ƒæ•´
        return []

    async def _search_video(self, params: Dict) -> str:
        """æœç´¢è§†é¢‘èµ„æº"""
        keyword = params.get("keyword", "")

        if not keyword:
            return "âŒ è¯·æä¾›æœç´¢å…³é”®è¯"

        logger.info(f"ğŸ¬ æœç´¢è§†é¢‘: {keyword}")

        # å›½å†…è§†é¢‘æœç´¢æº
        video_sources = [
            ("bilibili.com", "Bilibili", f"https://search.bilibili.com/all?keyword={urllib.parse.quote(keyword)}"),
            ("douyin.com", "æŠ–éŸ³", f"https://www.douyin.com/search/{urllib.parse.quote(keyword)}"),
            ("kuaishou.com", "å¿«æ‰‹", f"https://www.kuaishou.com/search/video?searchKey={urllib.parse.quote(keyword)}"),
            ("youku.com", "ä¼˜é…·", f"https://so.youku.com/search_video/q_{urllib.parse.quote(keyword)}"),
            ("iqiyi.com", "çˆ±å¥‡è‰º", f"https://so.iqiyi.com/so/q_{urllib.parse.quote(keyword)}"),
        ]

        result_text = f"ğŸ¬ è§†é¢‘æœç´¢ç»“æœ: {keyword}\n\n"
        for domain, name, url in video_sources:
            result_text += f"â€¢ {name}: {url}\n"

        return result_text

    async def _search_image(self, params: Dict) -> str:
        """æœç´¢å›¾ç‰‡èµ„æº"""
        keyword = params.get("keyword", "")

        if not keyword:
            return "âŒ è¯·æä¾›æœç´¢å…³é”®è¯"

        logger.info(f"ğŸ–¼ï¸ æœç´¢å›¾ç‰‡: {keyword}")

        # å›¾ç‰‡æœç´¢æº
        image_sources = [
            ("baidu.com", "ç™¾åº¦å›¾ç‰‡", f"https://image.baidu.com/search/index?tn=baiduimage&word={urllib.parse.quote(keyword)}"),
            ("bing.com", "å¿…åº”å›¾ç‰‡", f"https://www.bing.com/images/search?q={urllib.parse.quote(keyword)}"),
            ("sogou.com", "æœç‹—å›¾ç‰‡", f"https://pic.sogou.com/pics?query={urllib.parse.quote(keyword)}"),
            ("360.com", "360å›¾ç‰‡", f"https://image.so.com/i?q={urllib.parse.quote(keyword)}"),
        ]

        result_text = f"ğŸ–¼ï¸ å›¾ç‰‡æœç´¢ç»“æœ: {keyword}\n\n"
        for domain, name, url in image_sources:
            result_text += f"â€¢ {name}: {url}\n"

        return result_text

    async def _crawl_webpage(self, params: Dict) -> str:
        """æŠ“å–ç½‘é¡µå†…å®¹"""
        url = params.get("url", "")
        selector = params.get("selector", "")
        original_text = params.get("original_text", "")

        if not url:
            return "âŒ è¯·æä¾›ç½‘é¡µ URL"
        
        # å¦‚æœè¯·æ±‚åŒ…å«"é¡µé¢é“¾æ¥"æˆ–"æå–é“¾æ¥"ï¼Œè°ƒç”¨é“¾æ¥æå–æ–¹æ³•
        if "é¡µé¢é“¾æ¥" in original_text or "æå–é“¾æ¥" in original_text or "æ‰€æœ‰é“¾æ¥" in original_text:
            return await self._extract_page_links(params)
        
        # å¦‚æœè¯·æ±‚åŒ…å«"è§†é¢‘é“¾æ¥"ï¼Œè°ƒç”¨è§†é¢‘é“¾æ¥æå–æ–¹æ³•
        if "è§†é¢‘é“¾æ¥" in original_text or "MP4é“¾æ¥" in original_text.upper():
            return await self._scrape_links(params)

        logger.info(f"ğŸŒ æŠ“å–ç½‘é¡µ: {url}")

        try:
            req = urllib.request.Request(url, headers=self._get_headers())

            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: urllib.request.urlopen(req, timeout=20, context=self._create_ssl_context())
            )

            html = response.read().decode('utf-8', errors='ignore')

            # æå–æ ‡é¢˜
            title_match = re.search(r'<title[^>]*>([^<]+)</title>', html, re.IGNORECASE)
            title = title_match.group(1).strip() if title_match else "Unknown"

            # æå–æ‰€æœ‰é“¾æ¥
            links = re.findall(r'href=["\'](https?://[^"\']+)["\']', html)

            # æå–æ‰€æœ‰å›¾ç‰‡
            images = re.findall(r'src=["\'](https?://[^"\']+\.(?:jpg|jpeg|png|gif|webp))["\']', html, re.IGNORECASE)

            result = f"ğŸŒ ç½‘é¡µæŠ“å–ç»“æœ\n\n"
            result += f"æ ‡é¢˜: {title}\n"
            result += f"URL: {url}\n"
            result += f"å†…å®¹é•¿åº¦: {len(html)} å­—ç¬¦\n"
            result += f"é“¾æ¥æ•°é‡: {len(links)}\n"
            result += f"å›¾ç‰‡æ•°é‡: {len(images)}\n\n"

            if links:
                result += "ä¸»è¦é“¾æ¥:\n"
                for link in links[:10]:
                    result += f"  â€¢ {link}\n"

            return result

        except Exception as e:
            logger.error(f"ç½‘é¡µæŠ“å–å¤±è´¥: {e}")
            return f"âŒ ç½‘é¡µæŠ“å–å¤±è´¥: {str(e)}"

    async def _fetch_api(self, params: Dict) -> str:
        """è·å– API æ•°æ®"""
        url = params.get("url", "")
        method = params.get("method", "GET")
        headers = params.get("headers", {})
        data = params.get("data", None)

        if not url:
            return "âŒ è¯·æä¾› API URL"

        logger.info(f"ğŸ“¡ è¯·æ±‚ API: {url}")

        try:
            req_headers = self._get_headers()
            req_headers.update(headers)

            if data and isinstance(data, dict):
                data = json.dumps(data).encode('utf-8')
                req_headers['Content-Type'] = 'application/json'

            req = urllib.request.Request(url, data=data, headers=req_headers, method=method)

            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: urllib.request.urlopen(req, timeout=20, context=self._create_ssl_context())
            )

            content = response.read().decode('utf-8', errors='ignore')

            # å°è¯•è§£æ JSON
            try:
                json_data = json.loads(content)
                formatted = json.dumps(json_data, indent=2, ensure_ascii=False)
                return f"ğŸ“¡ API å“åº”\n\n```json\n{formatted[:2000]}\n```"
            except:
                return f"ğŸ“¡ API å“åº”\n\n{content[:2000]}"

        except Exception as e:
            logger.error(f"API è¯·æ±‚å¤±è´¥: {e}")
            return f"âŒ API è¯·æ±‚å¤±è´¥: {str(e)}"

    def _get_task_status(self, params: Dict) -> str:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        task_id = params.get("task_id", "")

        if not task_id or task_id not in self.tasks:
            return "âŒ ä»»åŠ¡ä¸å­˜åœ¨"

        task = self.tasks[task_id]

        result = f"ğŸ•·ï¸ ä»»åŠ¡çŠ¶æ€\n\n"
        result += f"ä»»åŠ¡ID: {task.task_id}\n"
        result += f"ç±»å‹: {task.task_type}\n"
        result += f"å…³é”®è¯: {task.keyword}\n"
        result += f"çŠ¶æ€: {task.status}\n"
        result += f"åˆ›å»ºæ—¶é—´: {task.created_at.strftime('%Y-%m-%d %H:%M:%S')}\n"

        if task.completed_at:
            result += f"å®Œæˆæ—¶é—´: {task.completed_at.strftime('%Y-%m-%d %H:%M:%S')}\n"

        if task.error_message:
            result += f"é”™è¯¯ä¿¡æ¯: {task.error_message}\n"

        result += f"ç»“æœæ•°é‡: {len(task.results)}\n"

        return result

    def _get_task_results(self, params: Dict) -> str:
        """è·å–ä»»åŠ¡ç»“æœ"""
        task_id = params.get("task_id", "")
        limit = params.get("limit", 10)

        if not task_id or task_id not in self.tasks:
            return "âŒ ä»»åŠ¡ä¸å­˜åœ¨"

        task = self.tasks[task_id]

        if not task.results:
            return "ğŸ“­ æš‚æ— ç»“æœ"

        result = f"ğŸ“‹ ä»»åŠ¡ç»“æœ ({len(task.results)} ä¸ª)\n\n"

        for i, r in enumerate(task.results[:limit], 1):
            result += f"{i}. {r.title}\n"
            result += f"   URL: {r.url}\n"
            result += f"   æ¥æº: {r.source}\n"
            if r.quality != "unknown":
                result += f"   è´¨é‡: {r.quality}\n"
            result += "\n"

        return result

    async def _save_results_to_file(self, task_id: str, results: List[SearchResult]):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        try:
            # åˆ›å»ºç›®å½•
            data_dir = Path("data") / "crawler"
            data_dir.mkdir(parents=True, exist_ok=True)

            # ä¿å­˜ä¸º JSON
            file_path = data_dir / f"{task_id}.json"

            data = {
                "task_id": task_id,
                "created_at": datetime.now().isoformat(),
                "results": [
                    {
                        "title": r.title,
                        "url": r.url,
                        "source": r.source,
                        "quality": r.quality,
                        "size": r.size,
                        "duration": r.duration,
                        "extra_info": r.extra_info
                    }
                    for r in results
                ]
            }

            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)

            logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {file_path}")

        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")

    def get_status(self) -> Dict:
        """è·å–æ™ºèƒ½ä½“çŠ¶æ€"""
        status = super().get_status()
        status.update({
            "total_tasks": len(self.tasks),
            "active_tasks": len(self.active_tasks),
            "completed_tasks": sum(1 for t in self.tasks.values() if t.status == "completed"),
            "failed_tasks": sum(1 for t in self.tasks.values() if t.status == "failed"),
        })
        return status

    def _get_help_info(self) -> str:
        """è·å–å¸®åŠ©ä¿¡æ¯"""
        return """## çˆ¬è™«æ™ºèƒ½ä½“

### åŠŸèƒ½è¯´æ˜
çˆ¬è™«æ™ºèƒ½ä½“ä¸“é—¨è´Ÿè´£ç½‘ç»œçˆ¬è™«ä»»åŠ¡ï¼Œå¯ä»¥æœç´¢ç½‘é¡µã€æŠ“å–å†…å®¹ã€ä¸‹è½½èµ„æºç­‰ã€‚

### æ”¯æŒçš„æ“ä½œ
- **ç½‘é¡µæœç´¢**ï¼šæœç´¢äº’è”ç½‘ä¸Šçš„ä¿¡æ¯
- **æŠ“å–é“¾æ¥**ï¼šä»ç½‘é¡µä¸­æå–é“¾æ¥
- **ä¸‹è½½èµ„æº**ï¼šä¸‹è½½è§†é¢‘ã€å›¾ç‰‡ç­‰æ–‡ä»¶
- **APIæ•°æ®è·å–**ï¼šè°ƒç”¨APIè·å–æ•°æ®

### ä½¿ç”¨ç¤ºä¾‹
- "æœç´¢ Python æ•™ç¨‹" - æœç´¢ç›¸å…³ç½‘é¡µ
- "æœç´¢æ–°é—» äººå·¥æ™ºèƒ½" - æœç´¢æ–°é—»èµ„è®¯
- "ç™¾åº¦ä¸€ä¸‹ å¤©æ°”" - ä½¿ç”¨ç™¾åº¦æœç´¢
- "å¸®æˆ‘æœä¸€ä¸‹ èœè°±" - æœç´¢ç›¸å…³ä¿¡æ¯
- "æŠ“å–é“¾æ¥ https://example.com" - æŠ“å–ç½‘é¡µä¸­çš„æ‰€æœ‰é“¾æ¥
- "ä¸‹è½½è§†é¢‘ [è§†é¢‘é“¾æ¥]" - ä¸‹è½½è§†é¢‘æ–‡ä»¶

### æ”¯æŒçš„æœç´¢å¼•æ“
- ç™¾åº¦
- è°·æ­Œ
- å¿…åº”
- æœç‹—

### æ³¨æ„äº‹é¡¹
- éƒ¨åˆ†ç½‘ç«™å¯èƒ½éœ€è¦ç­‰å¾…åŠ è½½
- ä¸‹è½½å¤§æ–‡ä»¶æ—¶è¯·è€å¿ƒç­‰å¾…
- è¯·éµå®ˆç½‘ç«™çš„ä½¿ç”¨æ¡æ¬¾"""
