"""
æ–°é—»èµ„è®¯æ™ºèƒ½ä½“ - æ”¶é›†å’Œæ•´ç†æ–°é—»èµ„è®¯
æ”¯æŒå¤šæºæ–°é—»æŠ“å–ã€æ‘˜è¦ç”Ÿæˆã€åˆ†ç±»æ•´ç†
"""
import asyncio
import json
import random
from datetime import datetime
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from urllib.parse import urljoin, urlparse

import aiohttp
from bs4 import BeautifulSoup
from loguru import logger

from ..base import BaseAgent, Task


@dataclass
class NewsItem:
    """æ–°é—»æ¡ç›®"""
    title: str
    url: str
    source: str
    publish_time: Optional[str] = None
    summary: Optional[str] = None
    content: Optional[str] = None
    category: Optional[str] = None
    image_url: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


class NewsAgent(BaseAgent):
    """æ–°é—»èµ„è®¯æ™ºèƒ½ä½“ - æ”¶é›†æ–°é—»èµ„è®¯"""
    
    KEYWORD_MAPPINGS = {
        "æ–°é—»": ("fetch_news", {}),
        "çƒ­ç‚¹": ("fetch_hot", {}),
        "ä»Šæ—¥æ–°é—»": ("fetch_news", {}),
        "ä»Šæ—¥çƒ­ç‚¹": ("fetch_hot", {}),
        "æœ€æ–°æ–°é—»": ("fetch_news", {}),
        "æœ€æ–°çƒ­ç‚¹": ("fetch_hot", {}),
        "çœ‹æ–°é—»": ("fetch_news", {}),
        "çœ‹çƒ­ç‚¹": ("fetch_hot", {}),
        "ç§‘æŠ€æ–°é—»": ("fetch_news", {"category": "tech"}),
        "è´¢ç»æ–°é—»": ("fetch_news", {"category": "finance"}),
    }
    
    # æ–°é—»æºé…ç½® - ä½¿ç”¨RSSæºæ›´ç¨³å®š
    NEWS_SOURCES = {
        "36æ°ª": {
            "url": "https://36kr.com/feed",
            "type": "rss",
        },
        "è™å—…": {
            "url": "https://www.huxiu.com/rss/0.xml",
            "type": "rss",
        },
        "å°‘æ•°æ´¾": {
            "url": "https://sspai.com/feed",
            "type": "rss",
        },
        "ITä¹‹å®¶": {
            "url": "https://www.ithome.com/rss/",
            "type": "rss",
        },
        "çˆ±èŒƒå„¿": {
            "url": "https://www.ifanr.com/feed",
            "type": "rss",
        },
        "é’›åª’ä½“": {
            "url": "https://www.tmtpost.com/rss.xml",
            "type": "rss",
        },
        "å¼€æºä¸­å›½": {
            "url": "https://www.oschina.net/news/rss",
            "type": "rss",
        },
        "InfoQ": {
            "url": "https://www.infoq.cn/feed",
            "type": "rss",
        },
    }
    
    # å¤‡ç”¨æ–°é—»APIæº
    API_SOURCES = [
        "https://www.zhihu.com/api/v3/feed/topstory",
    ]
    
    def __init__(self):
        super().__init__(
            name="news_agent",
            description="æ–°é—»èµ„è®¯æ™ºèƒ½ä½“ - æ”¶é›†å’Œæ•´ç†æ–°é—»èµ„è®¯"
        )
        
        self.register_capability(
            capability="get_news",
            description="è·å–æ–°é—»èµ„è®¯ã€‚å¯ä»¥è·å–çƒ­ç‚¹æ–°é—»æˆ–ç‰¹å®šç±»åˆ«çš„æ–°é—»ã€‚",
            parameters={
                "type": "object",
                "properties": {
                    "category": {
                        "type": "string",
                        "description": "æ–°é—»ç±»åˆ«ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚'ç§‘æŠ€'ã€'è´¢ç»'ã€'ä½“è‚²'",
                        "default": "çƒ­ç‚¹"
                    },
                    "count": {
                        "type": "integer",
                        "description": "è¿”å›æ–°é—»æ¡æ•°",
                        "default": 5
                    }
                },
                "required": []
            },
            category="news"
        )
        
        self.news_cache: List[NewsItem] = []
        self.cache_time: Optional[datetime] = None
        self.cache_duration = 1800
        self._session: Optional[aiohttp.ClientSession] = None
        self._llm_gateway = None
        
        logger.info("ğŸ“° æ–°é—»èµ„è®¯æ™ºèƒ½ä½“å·²åˆå§‹åŒ–")
    
    def _get_llm_gateway(self):
        """è·å– LLM ç½‘å…³"""
        if self._llm_gateway is None:
            from ...llm import LLMGateway
            from ...config import settings
            self._llm_gateway = LLMGateway(settings.llm)
        return self._llm_gateway
    
    async def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨ LLM"""
        llm = self._get_llm_gateway()
        messages = [{"role": "user", "content": prompt}]
        response = await llm.chat(messages)
        return response.content
    
    async def start(self):
        """å¯åŠ¨æ™ºèƒ½ä½“"""
        await super().start()
        
        # åˆ›å»º SSL ä¸Šä¸‹æ–‡ï¼Œå¿½ç•¥è¯ä¹¦éªŒè¯
        import ssl
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        timeout = aiohttp.ClientTimeout(total=30, connect=10)
        connector = aiohttp.TCPConnector(limit=10, limit_per_host=5, ssl=ssl_context)
        self._session = aiohttp.ClientSession(
            timeout=timeout,
            connector=connector,
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            }
        )
    
    async def stop(self):
        """åœæ­¢æ™ºèƒ½ä½“"""
        if self._session:
            await self._session.close()
        await super().stop()
    
    async def execute_task(self, task: Task) -> str:
        """æ‰§è¡Œä»»åŠ¡"""
        task_type = task.type
        action = task.params.get("action", "").lower()
        params = task.params
        
        if task_type == "get_news" or task_type == "fetch_news":
            action = "fetch_news"
        elif not action:
            action = "fetch_news"
        
        logger.info(f"ğŸ“° News Agent æ‰§è¡Œ: {action}")
        
        try:
            if action in ["fetch_news", "news_fetch", "get_news"]:
                return await self._fetch_news(
                    count=params.get("count", 10),
                    category=params.get("category"),
                    source=params.get("source")
                )
            elif action in ["fetch_hot", "news_hot"]:
                return await self._fetch_hot_news(count=params.get("count", 10))
            elif action in ["search_news", "news_search"]:
                return await self._search_news(
                    keyword=params.get("keyword"),
                    count=params.get("count", 10)
                )
            elif action == "get_categories":
                return await self._get_categories()
            elif action == "get_sources":
                return await self._get_sources()
            else:
                return f"âŒ æœªçŸ¥çš„æ“ä½œ: {action}"
        
        except Exception as e:
            logger.error(f"News Agent æ‰§è¡Œå¤±è´¥: {e}")
            return f"âŒ æ“ä½œå¤±è´¥: {str(e)}"
    
    async def _fetch_news(self, count: int = 20, category: Optional[str] = None, 
                         source: Optional[str] = None) -> str:
        """è·å–æ–°é—»èµ„è®¯"""
        # æ£€æŸ¥ç¼“å­˜
        if self._is_cache_valid() and self.news_cache:
            news_list = self._get_from_cache(count, category)
            return await self._generate_news_brief(news_list)
        
        all_news = []
        
        # ä»å¤šä¸ªæºæŠ“å–æ–°é—»
        sources_to_fetch = {source: config} if source and source in self.NEWS_SOURCES else self.NEWS_SOURCES
        
        for source_name, config in sources_to_fetch.items():
            try:
                # æ¯ä¸ªæºè·å–æ›´å¤šæ–°é—»
                news = await self._fetch_from_source(source_name, config, 15)
                all_news.extend(news)
                logger.info(f"ğŸ“° ä» {source_name} è·å– {len(news)} æ¡æ–°é—»")
            except Exception as e:
                logger.warning(f"ä» {source_name} è·å–æ–°é—»å¤±è´¥: {e}")
        
        # å¦‚æœRSSæºéƒ½å¤±è´¥äº†ï¼Œå°è¯•çŸ¥ä¹çƒ­æ¦œä½œä¸ºå¤‡ç”¨
        if not all_news:
            logger.info("ğŸ“° RSSæºè·å–å¤±è´¥ï¼Œå°è¯•çŸ¥ä¹çƒ­æ¦œä½œä¸ºå¤‡ç”¨...")
            try:
                zhihu_news = await self._fetch_zhihu_hot(count)
                all_news.extend(zhihu_news)
            except Exception as e:
                logger.warning(f"çŸ¥ä¹çƒ­æ¦œè·å–å¤±è´¥: {e}")
        
        # å»é‡ï¼ˆåŸºäºæ ‡é¢˜ï¼‰
        seen_titles = set()
        unique_news = []
        for news in all_news:
            if news.title and news.title not in seen_titles:
                seen_titles.add(news.title)
                unique_news.append(news)
        
        # è¿‡æ»¤ä»Šå¤©çš„æ–°é—»
        today = datetime.now().date()
        today_news = []
        for news in unique_news:
            if news.publish_time:
                try:
                    # å°è¯•è§£æå‘å¸ƒæ—¶é—´
                    from email.utils import parsedate_to_datetime
                    pub_date = parsedate_to_datetime(news.publish_time)
                    if pub_date.date() == today:
                        today_news.append(news)
                except:
                    # å¦‚æœæ— æ³•è§£ææ—¶é—´ï¼Œä¿ç•™è¯¥æ–°é—»ï¼ˆå¯èƒ½æ˜¯æœ€æ–°å‘å¸ƒçš„ï¼‰
                    today_news.append(news)
            else:
                # æ²¡æœ‰å‘å¸ƒæ—¶é—´çš„æ–°é—»ä¹Ÿä¿ç•™
                today_news.append(news)
        
        # å¦‚æœä»Šå¤©çš„æ–°é—»å¤ªå°‘ï¼Œä½¿ç”¨æ‰€æœ‰æ–°é—»
        if len(today_news) < 3:
            today_news = unique_news
            title_suffix = "æœ€æ–°æ–°é—»èµ„è®¯"
        else:
            today_news = today_news
            title_suffix = f"ä»Šæ—¥æ–°é—» ({today.strftime('%Y-%m-%d')})"
        
        # æ›´æ–°ç¼“å­˜
        self.news_cache = today_news
        self.cache_time = datetime.now()
        
        # è¿”å›æŒ‡å®šæ•°é‡çš„æ–°é—»ï¼Œä½¿ç”¨LLMç”Ÿæˆç®€æŠ¥
        result_news = today_news[:count]
        return await self._generate_news_brief(result_news)
    
    async def _fetch_hot_news(self, count: int = 10) -> str:
        """è·å–çƒ­ç‚¹æ–°é—»"""
        # å°è¯•ä»å¤šä¸ªæºè·å–çƒ­é—¨æ–°é—»
        hot_news = []
        
        # çŸ¥ä¹çƒ­æ¦œ
        try:
            zhihu_hot = await self._fetch_zhihu_hot(count)
            hot_news.extend(zhihu_hot)
        except Exception as e:
            logger.warning(f"è·å–çŸ¥ä¹çƒ­æ¦œå¤±è´¥: {e}")
        
        # å¦‚æœçŸ¥ä¹çƒ­æ¦œä¸å¤Ÿï¼Œä»æ–°é—»æºè¡¥å……
        if len(hot_news) < count:
            try:
                await self._fetch_news(count)
                hot_news.extend(self.news_cache[:count - len(hot_news)])
            except Exception as e:
                logger.warning(f"è·å–æ–°é—»å¤±è´¥: {e}")
        
        # å»é‡å¹¶æ’åº
        seen = set()
        unique_hot = []
        for news in hot_news:
            if news.title not in seen:
                seen.add(news.title)
                unique_hot.append(news)
        
        # ä½¿ç”¨LLMç”Ÿæˆçƒ­ç‚¹ç®€æŠ¥
        return await self._generate_hot_brief(unique_hot[:count])
    
    async def _fetch_zhihu_hot(self, count: int = 10) -> List[NewsItem]:
        """è·å–çŸ¥ä¹çƒ­æ¦œ"""
        url = "https://www.zhihu.com/api/v3/feed/topstory/hot-lists/total"
        
        try:
            async with self._session.get(url) as response:
                if response.status != 200:
                    return []
                
                data = await response.json()
                news_list = []
                
                for item in data.get("data", [])[:count]:
                    target = item.get("target", {})
                    news = NewsItem(
                        title=target.get("title", ""),
                        url=target.get("url", ""),
                        source="çŸ¥ä¹çƒ­æ¦œ",
                        summary=target.get("excerpt", "")[:100] + "..." if target.get("excerpt") else None
                    )
                    news_list.append(news)
                
                return news_list
        except Exception as e:
            logger.error(f"è·å–çŸ¥ä¹çƒ­æ¦œå¤±è´¥: {e}")
            return []
    
    async def _fetch_from_source(self, source_name: str, config: Dict, count: int) -> List[NewsItem]:
        """ä»æŒ‡å®šæºæŠ“å–æ–°é—»"""
        news_list = []
        
        try:
            async with self._session.get(config["url"]) as response:
                if response.status != 200:
                    logger.warning(f"{source_name} è¿”å›çŠ¶æ€ç : {response.status}")
                    return []
                
                content = await response.text()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯RSSæº
                if config.get("type") == "rss" or "rss" in config["url"] or "feed" in config["url"]:
                    news_list = self._parse_rss(content, source_name, count)
                else:
                    # HTMLè§£æ
                    soup = BeautifulSoup(content, 'html.parser')
                    links = soup.select(config.get("selector", "a"))
                    
                    for link in links[:count]:
                        title = link.get_text(strip=True)
                        href = link.get('href', '')
                        
                        if not title or not href:
                            continue
                        
                        # å¤„ç†ç›¸å¯¹URL
                        if href.startswith('//'):
                            href = 'https:' + href
                        elif href.startswith('/'):
                            href = urljoin(config.get("base_url", ""), href)
                        elif not href.startswith('http'):
                            href = urljoin(config.get("base_url", ""), href)
                        
                        news = NewsItem(
                            title=title,
                            url=href,
                            source=source_name
                        )
                        news_list.append(news)
        
        except Exception as e:
            logger.error(f"ä» {source_name} æŠ“å–æ–°é—»å¤±è´¥: {e}")
        
        return news_list
    
    def _parse_rss(self, content: str, source_name: str, count: int) -> List[NewsItem]:
        """è§£æRSS/Atomè®¢é˜…"""
        news_list = []
        
        try:
            soup = BeautifulSoup(content, 'xml')
            
            # å°è¯•RSSæ ¼å¼
            items = soup.find_all('item')
            if not items:
                # å°è¯•Atomæ ¼å¼
                items = soup.find_all('entry')
            
            for item in items[:count]:
                # è·å–æ ‡é¢˜
                title_tag = item.find('title')
                title = title_tag.get_text(strip=True) if title_tag else ""
                
                # è·å–é“¾æ¥
                link_tag = item.find('link')
                if link_tag:
                    href = link_tag.get('href') or link_tag.get_text(strip=True)
                else:
                    href = ""
                
                # è·å–æè¿°/æ‘˜è¦
                desc_tag = item.find('description') or item.find('summary')
                summary = ""
                if desc_tag:
                    summary = desc_tag.get_text(strip=True)[:200]
                    if len(summary) > 100:
                        summary = summary[:100] + "..."
                
                # è·å–å‘å¸ƒæ—¶é—´
                time_tag = item.find('pubDate') or item.find('published') or item.find('updated')
                pub_time = time_tag.get_text(strip=True) if time_tag else None
                
                if title and href:
                    news = NewsItem(
                        title=title,
                        url=href,
                        source=source_name,
                        summary=summary if summary else None,
                        publish_time=pub_time
                    )
                    news_list.append(news)
            
            logger.info(f"ğŸ“° ä» {source_name} RSSè§£æåˆ° {len(news_list)} æ¡æ–°é—»")
            
        except Exception as e:
            logger.error(f"è§£æRSSå¤±è´¥: {e}")
        
        return news_list
    
    async def _search_news(self, keyword: Optional[str], count: int = 10) -> str:
        """æœç´¢æ–°é—»"""
        if not keyword:
            return "âŒ è¯·æä¾›æœç´¢å…³é”®è¯"
        
        # å…ˆè·å–æœ€æ–°æ–°é—»
        await self._fetch_news(count * 2)
        
        # è¿‡æ»¤åŒ…å«å…³é”®è¯çš„æ–°é—»
        filtered = [
            news for news in self.news_cache
            if keyword.lower() in news.title.lower() or 
               (news.summary and keyword.lower() in news.summary.lower())
        ]
        
        if not filtered:
            return f"ğŸ” æœªæ‰¾åˆ°åŒ…å« '{keyword}' çš„æ–°é—»"
        
        return self._format_news_output(filtered[:count], f"ğŸ” æœç´¢ '{keyword}' çš„ç»“æœ (å…±{len(filtered[:count])}æ¡)")
    
    async def _get_categories(self) -> str:
        """è·å–æ–°é—»åˆ†ç±»"""
        categories = ["å›½å†…", "å›½é™…", "è´¢ç»", "ç§‘æŠ€", "ä½“è‚²", "å¨±ä¹", "ç¤¾ä¼š", "å†›äº‹"]
        return "ğŸ“‚ æ–°é—»åˆ†ç±»:\n" + "\n".join(f"  â€¢ {cat}" for cat in categories)
    
    async def _get_sources(self) -> str:
        """è·å–æ–°é—»æº"""
        sources = list(self.NEWS_SOURCES.keys())
        return "ğŸ“¡ æ–°é—»æº:\n" + "\n".join(f"  â€¢ {source}" for source in sources)
    
    def _is_cache_valid(self) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not self.cache_time or not self.news_cache:
            return False
        elapsed = (datetime.now() - self.cache_time).total_seconds()
        return elapsed < self.cache_duration
    
    def _get_from_cache(self, count: int, category: Optional[str] = None) -> List[NewsItem]:
        """ä»ç¼“å­˜è·å–æ–°é—»"""
        news_list = self.news_cache
        if category:
            news_list = [n for n in news_list if n.category == category]
        return news_list[:count]
    
    def _format_news_output(self, news_list: List[NewsItem], title: str) -> str:
        """æ ¼å¼åŒ–æ–°é—»è¾“å‡º"""
        if not news_list:
            return "ğŸ“° æš‚æ— æ–°é—»èµ„è®¯"
        
        lines = [f"{title}\n"]
        
        for i, news in enumerate(news_list, 1):
            lines.append(f"{i}. {news.title}")
            if news.summary:
                lines.append(f"   ğŸ“ {news.summary}")
            lines.append(f"   ğŸ“¡ {news.source}")
            if news.url:
                lines.append(f"   ğŸ”— {news.url}")
            lines.append("")
        
        return "\n".join(lines)
    
    async def _generate_news_brief(self, news_list: List[NewsItem]) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆæ–°é—»ç®€æŠ¥"""
        if not news_list:
            return "ğŸ“° æš‚æ— æ–°é—»èµ„è®¯"
        
        # æ„å»ºæ–°é—»å†…å®¹
        news_content = "\n".join([
            f"{i+1}. ã€{n.source}ã€‘{n.title}\n   {n.summary or ''}"
            for i, n in enumerate(news_list[:30])
        ])
        
        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹æ–°é—»å†…å®¹ï¼Œç”Ÿæˆä¸€ä»½ç®€æ´çš„æ–°é—»ç®€æŠ¥ã€‚

æ–°é—»å†…å®¹ï¼š
{news_content}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

## ğŸ“° ä»Šæ—¥æ–°é—»ç®€æŠ¥

### ğŸ”¥ çƒ­ç‚¹å…³æ³¨
ï¼ˆåˆ—å‡º3-5æ¡æœ€é‡è¦çš„æ–°é—»ï¼Œæ¯æ¡ç”¨ä¸€å¥è¯æ¦‚æ‹¬ï¼‰

### ğŸ“Š è¡Œä¸šåŠ¨æ€
ï¼ˆç§‘æŠ€ã€è´¢ç»ç­‰é¢†åŸŸçš„æ–°é—»æ‘˜è¦ï¼‰

### ğŸ’¡ ç®€æŠ¥æ€»ç»“
ï¼ˆç”¨2-3å¥è¯æ€»ç»“ä»Šå¤©çš„ä¸»è¦æ–°é—»è¶‹åŠ¿ï¼‰

æ³¨æ„ï¼š
1. æå–æ ¸å¿ƒä¿¡æ¯ï¼Œå»é™¤å†—ä½™å†…å®¹
2. ä¿æŒç®€æ´ï¼Œæ¯æ¡æ‘˜è¦ä¸è¶…è¿‡50å­—
3. æŒ‰é‡è¦æ€§æ’åº
4. ä½¿ç”¨ç®€æ´çš„ä¸­æ–‡è¡¨è¾¾"""

        try:
            brief = await self._call_llm(prompt)
            return brief
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ–°é—»ç®€æŠ¥å¤±è´¥: {e}")
            return self._format_news_output(news_list, "ğŸ“° ä»Šæ—¥æ–°é—»")
    
    async def _generate_hot_brief(self, news_list: List[NewsItem]) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆçƒ­ç‚¹æ–°é—»ç®€æŠ¥"""
        if not news_list:
            return "ğŸ”¥ æš‚æ— çƒ­ç‚¹æ–°é—»"
        
        # æ„å»ºæ–°é—»å†…å®¹
        news_content = "\n".join([
            f"{i+1}. {n.title}\n   {n.summary or ''}"
            for i, n in enumerate(news_list[:20])
        ])
        
        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹çƒ­ç‚¹æ–°é—»å†…å®¹ï¼Œç”Ÿæˆä¸€ä»½ç®€æ´çš„çƒ­ç‚¹ç®€æŠ¥ã€‚

çƒ­ç‚¹å†…å®¹ï¼š
{news_content}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

## ğŸ”¥ ä»Šæ—¥çƒ­ç‚¹

### ğŸ“Œ çƒ­ç‚¹é€Ÿè§ˆ
ï¼ˆç”¨ä¸€å¥è¯æ¦‚æ‹¬æ¯æ¡çƒ­ç‚¹ï¼ŒæŒ‰çƒ­åº¦æ’åºï¼‰

### ğŸ’¬ çƒ­ç‚¹è§£è¯»
ï¼ˆæŒ‘é€‰2-3ä¸ªæœ€é‡è¦çš„çƒ­ç‚¹è¿›è¡Œç®€è¦è§£è¯»ï¼‰

æ³¨æ„ï¼š
1. çªå‡ºé‡ç‚¹ï¼Œç®€æ´æ˜äº†
2. æ¯æ¡çƒ­ç‚¹æ¦‚æ‹¬ä¸è¶…è¿‡30å­—
3. ä¿ç•™æ–°é—»çš„æ ¸å¿ƒä¿¡æ¯"""

        try:
            brief = await self._call_llm(prompt)
            return brief
        except Exception as e:
            logger.error(f"ç”Ÿæˆçƒ­ç‚¹ç®€æŠ¥å¤±è´¥: {e}")
            return self._format_news_output(news_list, "ğŸ”¥ çƒ­ç‚¹æ–°é—»")
    
    def get_capabilities(self) -> list:
        """è·å–èƒ½åŠ›åˆ—è¡¨"""
        return [
            "news_fetch",
            "news_search",
            "hot_news",
            "news_summary"
        ]
