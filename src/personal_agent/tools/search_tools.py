"""
Search Tools - è”ç½‘æœç´¢å·¥å…·ï¼ˆå¸¦æ¥æºéªŒè¯ï¼‰
"""
import json
import re
from typing import Any, Dict, List, Optional
from urllib.parse import urlparse

from .base import BaseTool, ToolResult, tool_registry

AUTHORITATIVE_DOMAINS = {
    "news": [
        "xinhuanet.com",
        "people.com.cn", 
        "cctv.com",
        "chinadaily.com.cn",
        "thepaper.cn",
        "caixin.com",
        "reuters.com",
        "bbc.com",
        "cnn.com",
        "bloomberg.com",
    ],
    "tech": [
        "github.com",
        "stackoverflow.com",
        "stackoverflow.cn",
        "csdn.net",
        "juejin.cn",
        "zhihu.com",
        "segmentfault.com",
    ],
    "government": [
        "gov.cn",
        "stats.gov.cn",
        "pbc.gov.cn",
        "mof.gov.cn",
        "ndrc.gov.cn",
    ],
    "finance": [
        "sse.com.cn",
        "szse.cn",
        "csrc.gov.cn",
        "eastmoney.com",
        "sina.com.cn",
        "10jqka.com.cn",
    ],
    "academic": [
        "cnki.net",
        "wanfangdata.com.cn",
        "scholar.google.com",
        "arxiv.org",
        "nature.com",
        "science.org",
    ],
    "weather": [
        "weather.com.cn",
        "cma.gov.cn",
        "tianqi.com",
    ],
    "software": [
        "pypi.org",
        "npmjs.com",
        "apps.microsoft.com",
        "winget.run",
    ],
}

SUSPICIOUS_PATTERNS = [
    r"clickbait",
    r"éœ‡æƒŠ",
    r"å¿…çœ‹",
    r"è½¬å‘æœ‰å¥–",
    r"é™æ—¶å…è´¹",
    r"å†…éƒ¨æ¶ˆæ¯",
]


class WebSearchTool(BaseTool):
    name = "web_search"
    description = """è”ç½‘æœç´¢å·¥å…·ã€‚å½“ç”¨æˆ·è¯¢é—®éœ€è¦å®æ—¶ä¿¡æ¯ã€æœ€æ–°æ–°é—»ã€å½“å‰äº‹ä»¶ã€å¤©æ°”ã€è‚¡ä»·ç­‰æ—¶æ•ˆæ€§å†…å®¹æ—¶ä½¿ç”¨ã€‚
    
ã€é‡è¦ã€‘æœç´¢åŸåˆ™ï¼š
1. ä¸è¦ç”¨äºå¸¸è¯†æ€§é—®é¢˜
2. æœç´¢ç»“æœä¼šæ ‡æ³¨æ¥æºå¯ä¿¡åº¦
3. ä¼˜å…ˆä½¿ç”¨æƒå¨æ¥æºçš„æ•°æ®
4. å¦‚æœæœç´¢ç»“æœä¸å¯é ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·"""
    parameters = {
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "æœç´¢å…³é”®è¯ï¼Œåº”è¯¥ç®€æ´æ˜ç¡®"
            },
            "category": {
                "type": "string",
                "description": "æœç´¢ç±»åˆ«ï¼ˆå¯é€‰ï¼‰ï¼šnews/tech/government/finance/academic/weather/software",
                "enum": ["news", "tech", "government", "finance", "academic", "weather", "software"]
            }
        },
        "required": ["query"]
    }

    async def execute(self, query: str, category: Optional[str] = None) -> ToolResult:
        try:
            import dashscope
            from dashscope import Generation

            search_prompt = f"""è¯·æœç´¢ä»¥ä¸‹é—®é¢˜ï¼Œå¹¶æä¾›è¯¦ç»†çš„æœç´¢ç»“æœã€‚

ã€é‡è¦è¦æ±‚ã€‘
1. å¿…é¡»æ ‡æ³¨æ¯æ¡ä¿¡æ¯çš„æ¥æºï¼ˆç½‘ç«™åç§°å’Œé“¾æ¥ï¼‰
2. ä¼˜å…ˆå±•ç¤ºæƒå¨æ¥æºçš„ä¿¡æ¯
3. å¦‚æœä¿¡æ¯æ¥æºä¸å¯é ï¼Œè¯·æ˜ç¡®æ ‡æ³¨"æ¥æºå¾…éªŒè¯"
4. ä¸è¦ç¼–é€ ä»»ä½•æ•°æ®æˆ–ä¿¡æ¯

æœç´¢é—®é¢˜ï¼š{query}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¿”å›ï¼š
---
## æœç´¢ç»“æœ

### æ¥æº1: [ç½‘ç«™åç§°] (å¯ä¿¡åº¦: é«˜/ä¸­/ä½)
- é“¾æ¥: [URL]
- å†…å®¹: [æ‘˜è¦]

### æ¥æº2: [ç½‘ç«™åç§°] (å¯ä¿¡åº¦: é«˜/ä¸­/ä½)
- é“¾æ¥: [URL]  
- å†…å®¹: [æ‘˜è¦]

---
## ç»“è®º
[åŸºäºå¯é æ¥æºçš„æ€»ç»“]
"""
            
            response = Generation.call(
                model="qwen-plus",
                messages=[{"role": "user", "content": search_prompt}],
                enable_search=True,
                result_format="message"
            )

            if response.status_code != 200:
                return ToolResult(
                    success=False,
                    output="",
                    error=f"æœç´¢å¤±è´¥: {response.message}"
                )

            content = response.output.choices[0].message.content
            
            verified_content = self._verify_sources(content, category)

            return ToolResult(
                success=True,
                output=verified_content,
                data={
                    "query": query, 
                    "source": "web_search",
                    "category": category,
                    "verified": True
                }
            )

        except Exception as e:
            return ToolResult(
                success=False,
                output="",
                error=f"æœç´¢å‡ºé”™: {str(e)}"
            )
    
    def _verify_sources(self, content: str, category: Optional[str] = None) -> str:
        """éªŒè¯æ¥æºå¯ä¿¡åº¦"""
        warning = "\n\nâš ï¸ ã€æ•°æ®æ¥æºè¯´æ˜ã€‘\n"
        warning += "- ä»¥ä¸Šä¿¡æ¯æ¥è‡ªç½‘ç»œæœç´¢ï¼Œè¯·æ ¸å®é‡è¦ä¿¡æ¯\n"
        warning += "- æƒå¨æ¥æºï¼šæ”¿åºœç½‘ç«™ã€å®˜æ–¹åª’ä½“ã€å­¦æœ¯æœºæ„\n"
        warning += "- å¦‚éœ€ç¡®è®¤ï¼Œå»ºè®®è®¿é—®åŸå§‹æ¥æºæŸ¥çœ‹å®Œæ•´å†…å®¹\n"
        
        if category and category in AUTHORITATIVE_DOMAINS:
            domains = AUTHORITATIVE_DOMAINS[category]
            warning += f"- è¯¥ç±»åˆ«æ¨èæ¥æºï¼š{', '.join(domains[:5])}\n"
        
        for pattern in SUSPICIOUS_PATTERNS:
            if re.search(pattern, content, re.IGNORECASE):
                warning += f"\nâš ï¸ æ£€æµ‹åˆ°å¯ç–‘å†…å®¹æ¨¡å¼ï¼Œè¯·è°¨æ…å¯¹å¾…\n"
                break
        
        return content + warning


class WebFetchTool(BaseTool):
    name = "web_fetch"
    description = """è·å–ç½‘é¡µå†…å®¹ã€‚å½“éœ€è¦è¯»å–ç‰¹å®šç½‘é¡µçš„è¯¦ç»†å†…å®¹æ—¶ä½¿ç”¨ã€‚

ã€é‡è¦ã€‘ä½¿ç”¨åŸåˆ™ï¼š
1. ä¼˜å…ˆè·å–æƒå¨ç½‘ç«™çš„å†…å®¹
2. ä¼šè‡ªåŠ¨è¯„ä¼°æ¥æºå¯ä¿¡åº¦
3. å¯¹äºå¯ç–‘ç½‘ç«™ä¼šå‘å‡ºè­¦å‘Š"""
    parameters = {
        "type": "object",
        "properties": {
            "url": {
                "type": "string",
                "description": "è¦è·å–çš„ç½‘é¡µURL"
            }
        },
        "required": ["url"]
    }

    async def execute(self, url: str) -> ToolResult:
        try:
            credibility = self._check_credibility(url)
            
            import aiohttp
            from bs4 import BeautifulSoup

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }

            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=10, headers=headers) as response:
                    if response.status != 200:
                        return ToolResult(
                            success=False,
                            output="",
                            error=f"HTTP {response.status}"
                        )

                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    for script in soup(["script", "style", "nav", "footer", "aside"]):
                        script.decompose()

                    title = soup.title.string if soup.title else "æ— æ ‡é¢˜"
                    
                    text = soup.get_text(separator='\n', strip=True)
                    
                    if len(text) > 8000:
                        text = text[:8000] + "...\n\n[å†…å®¹å·²æˆªæ–­ï¼Œè¯·è®¿é—®åŸç½‘é¡µæŸ¥çœ‹å®Œæ•´å†…å®¹]"

                    result = f"ğŸ“„ ç½‘é¡µæ ‡é¢˜: {title}\n"
                    result += f"ğŸ”— æ¥æº: {url}\n"
                    result += f"âœ… å¯ä¿¡åº¦: {credibility['level']} ({credibility['reason']})\n"
                    result += f"\n{'='*50}\n\n"
                    result += text
                    
                    if credibility['level'] in ['ä½', 'æœªçŸ¥']:
                        result += "\n\nâš ï¸ ã€è­¦å‘Šã€‘æ­¤æ¥æºå¯ä¿¡åº¦è¾ƒä½ï¼Œè¯·è°¨æ…å¯¹å¾…å†…å®¹ï¼\n"
                        result += "å»ºè®®ï¼šäº¤å‰éªŒè¯ä¿¡æ¯ï¼Œæˆ–å¯»æ‰¾æ›´æƒå¨çš„æ¥æºã€‚\n"

                    return ToolResult(
                        success=True,
                        output=result,
                        data={
                            "url": url, 
                            "length": len(text),
                            "credibility": credibility
                        }
                    )

        except Exception as e:
            return ToolResult(
                success=False,
                output="",
                error=f"è·å–ç½‘é¡µå¤±è´¥: {str(e)}"
            )
    
    def _check_credibility(self, url: str) -> Dict[str, str]:
        """æ£€æŸ¥URLçš„å¯ä¿¡åº¦"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            
            for category, domains in AUTHORITATIVE_DOMAINS.items():
                for auth_domain in domains:
                    if auth_domain in domain:
                        return {
                            "level": "é«˜",
                            "reason": f"æƒå¨æ¥æºï¼ˆ{category}ç±»åˆ«ï¼‰"
                        }
            
            if domain.endswith('.gov.cn'):
                return {
                    "level": "é«˜",
                    "reason": "æ”¿åºœå®˜æ–¹ç½‘ç«™"
                }
            
            if domain.endswith('.edu.cn'):
                return {
                    "level": "é«˜",
                    "reason": "æ•™è‚²æœºæ„ç½‘ç«™"
                }
            
            if 'wiki' in domain:
                return {
                    "level": "ä¸­",
                    "reason": "ç»´åŸºç±»ç½‘ç«™ï¼Œéœ€è¦äº¤å‰éªŒè¯"
                }
            
            if any(p in domain for p in ['blog', 'bbs', 'forum', 'weibo', 'twitter', 'facebook']):
                return {
                    "level": "ä½",
                    "reason": "ç¤¾äº¤åª’ä½“/åšå®¢ï¼Œå†…å®¹å¯èƒ½ä¸å‡†ç¡®"
                }
            
            return {
                "level": "æœªçŸ¥",
                "reason": "æœªçŸ¥çš„æ¥æºï¼Œè¯·è°¨æ…å¯¹å¾…"
            }
            
        except Exception:
            return {
                "level": "æœªçŸ¥",
                "reason": "æ— æ³•è§£ææ¥æº"
            }


class SourceVerifyTool(BaseTool):
    name = "verify_source"
    description = "éªŒè¯ä¿¡æ¯æ¥æºçš„å¯ä¿¡åº¦ã€‚è¾“å…¥URLæˆ–ä¿¡æ¯å†…å®¹ï¼Œè¿”å›å¯ä¿¡åº¦è¯„ä¼°ã€‚"
    parameters = {
        "type": "object",
        "properties": {
            "url": {
                "type": "string",
                "description": "è¦éªŒè¯çš„URLï¼ˆå¯é€‰ï¼‰"
            },
            "content": {
                "type": "string",
                "description": "è¦éªŒè¯çš„ä¿¡æ¯å†…å®¹ï¼ˆå¯é€‰ï¼‰"
            }
        }
    }

    async def execute(self, url: Optional[str] = None, content: Optional[str] = None) -> ToolResult:
        result = "ğŸ“‹ æ¥æºéªŒè¯æŠ¥å‘Š\n\n"
        
        if url:
            fetch_tool = WebFetchTool()
            credibility = fetch_tool._check_credibility(url)
            result += f"ğŸ”— URL: {url}\n"
            result += f"âœ… å¯ä¿¡åº¦: {credibility['level']}\n"
            result += f"ğŸ“ åŸå› : {credibility['reason']}\n\n"
            
            if credibility['level'] in ['ä½', 'æœªçŸ¥']:
                result += "âš ï¸ å»ºè®®ï¼šå¯»æ‰¾æ›´æƒå¨çš„æ¥æºéªŒè¯æ­¤ä¿¡æ¯\n"
        
        if content:
            result += "ğŸ“„ å†…å®¹åˆ†æ:\n"
            
            suspicious_found = []
            for pattern in SUSPICIOUS_PATTERNS:
                if re.search(pattern, content, re.IGNORECASE):
                    suspicious_found.append(pattern)
            
            if suspicious_found:
                result += f"âš ï¸ æ£€æµ‹åˆ°å¯ç–‘æ¨¡å¼: {', '.join(suspicious_found)}\n"
                result += "å»ºè®®ï¼šè°¨æ…å¯¹å¾…æ­¤ä¿¡æ¯ï¼Œå¯»æ‰¾æƒå¨æ¥æºéªŒè¯\n"
            else:
                result += "âœ… æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„å¯ç–‘æ¨¡å¼\n"
        
        return ToolResult(
            success=True,
            output=result,
            data={"verified": True}
        )


def register_search_tools():
    tool_registry.register(WebSearchTool())
    tool_registry.register(WebFetchTool())
    tool_registry.register(SourceVerifyTool())
