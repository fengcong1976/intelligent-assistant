"""
LLM å·¥ä½œæµè§„åˆ’å™¨ - ä½¿ç”¨ LLM åˆ†æä»»åŠ¡ä¾èµ–å…³ç³»

è®© LLM ç†è§£ä»»åŠ¡è¯­ä¹‰ï¼Œè§„åˆ’å·¥ä½œæµæ‰§è¡Œé¡ºåº
"""
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
from loguru import logger
import json

from ..llm.gateway import LLMGateway
from ..config import settings
from .workflow_planner import WorkflowPlanner, WorkflowPlan, WorkflowNode, ExecutionMode


WORKFLOW_PLANNING_PROMPT = """ä½ æ˜¯ä¸€ä¸ªå·¥ä½œæµè§„åˆ’ä¸“å®¶ã€‚åˆ†æç”¨æˆ·çš„ä»»åŠ¡è¯·æ±‚ï¼Œè§„åˆ’å·¥å…·è°ƒç”¨çš„æ‰§è¡Œé¡ºåºã€‚

## å¯ç”¨å·¥å…·

{tools_description}

## ä»»åŠ¡è¯·æ±‚

{user_request}

## LLM å·²é€‰æ‹©çš„å·¥å…·è°ƒç”¨

{tool_calls_json}

## è§„åˆ’è§„åˆ™

1. **å¹¶è¡Œæ‰§è¡Œ**ï¼šå¤šä¸ªå·¥å…·ä¹‹é—´æ²¡æœ‰ä¾èµ–å…³ç³»æ—¶ï¼Œå¯ä»¥åŒæ—¶æ‰§è¡Œ
2. **ä¸²è¡Œæ‰§è¡Œ**ï¼šåä¸€ä¸ªå·¥å…·éœ€è¦å‰ä¸€ä¸ªå·¥å…·çš„è¾“å‡ºæ—¶ï¼Œå¿…é¡»ç­‰å¾…
3. **æ•°æ®ä¾èµ–**ï¼šå¦‚æœå·¥å…·å‚æ•°ä¸­æœ‰ç©ºå€¼æˆ–å ä½ç¬¦ï¼ˆå¦‚ `/path/to/`ï¼‰ï¼Œéœ€è¦ä¾èµ–å‰åºå·¥å…·å¡«å……
4. **æ–‡ä»¶ä¾èµ–**ï¼šå‘é€é‚®ä»¶çš„é™„ä»¶å¿…é¡»æ¥è‡ªæ–‡æ¡£ç”Ÿæˆå·¥å…·çš„è¾“å‡º

## è¾“å‡ºæ ¼å¼

è¯·è¾“å‡º JSON æ ¼å¼çš„å·¥ä½œæµè®¡åˆ’ï¼š

```json
{{
  "analysis": "ä»»åŠ¡åˆ†æè¯´æ˜",
  "execution_plan": [
    {{
      "step": 1,
      "parallel": false,
      "tools": ["tool_name"],
      "reason": "æ‰§è¡ŒåŸå› "
    }},
    {{
      "step": 2,
      "parallel": true,
      "tools": ["tool_a", "tool_b"],
      "reason": "è¿™ä¸¤ä¸ªå·¥å…·å¯ä»¥å¹¶è¡Œæ‰§è¡Œ"
    }}
  ],
  "dependencies": {{
    "tool_name": ["depends_on_tool"]
  }}
}}
```

åªè¾“å‡º JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""


@dataclass
class LLMWorkflowPlan:
    analysis: str
    execution_plan: List[Dict]
    dependencies: Dict[str, List[str]]
    raw_response: str


class LLMWorkflowPlanner:
    
    def __init__(self, llm: Optional[LLMGateway] = None):
        self.llm = llm or LLMGateway(settings.llm)
        self.rule_planner = WorkflowPlanner()
    
    async def plan_workflow(
        self,
        user_request: str,
        tool_calls: List[Dict],
        tools_description: str = ""
    ) -> LLMWorkflowPlan:
        """
        ä½¿ç”¨ LLM è§„åˆ’å·¥ä½œæµ
        
        Args:
            user_request: ç”¨æˆ·åŸå§‹è¯·æ±‚
            tool_calls: LLM è¿”å›çš„å·¥å…·è°ƒç”¨åˆ—è¡¨
            tools_description: å¯ç”¨å·¥å…·æè¿°
            
        Returns:
            LLMWorkflowPlan: LLM è§„åˆ’çš„å·¥ä½œæµ
        """
        if len(tool_calls) <= 1:
            return LLMWorkflowPlan(
                analysis="å•ä¸ªå·¥å…·è°ƒç”¨ï¼Œæ— éœ€è§„åˆ’",
                execution_plan=[{"step": 1, "parallel": False, "tools": [tool_calls[0]["name"]]}],
                dependencies={},
                raw_response=""
            )
        
        prompt = WORKFLOW_PLANNING_PROMPT.format(
            tools_description=tools_description or self._get_default_tools_description(),
            user_request=user_request,
            tool_calls_json=json.dumps(tool_calls, ensure_ascii=False, indent=2)
        )
        
        try:
            response = await self.llm.chat([{"role": "user", "content": prompt}])
            
            plan = self._parse_llm_response(response.content)
            
            logger.info(f"ğŸ¤– LLM å·¥ä½œæµè§„åˆ’å®Œæˆ: {plan.analysis}")
            return plan
            
        except Exception as e:
            logger.error(f"LLM å·¥ä½œæµè§„åˆ’å¤±è´¥: {e}")
            return self._fallback_to_rule_planner(tool_calls)
    
    def _parse_llm_response(self, response: str) -> LLMWorkflowPlan:
        """è§£æ LLM è¿”å›çš„ JSON"""
        import re
        
        json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response)
        if json_match:
            json_str = json_match.group(1)
        else:
            json_str = response
        
        json_str = re.sub(r'```.*?```', '', json_str, flags=re.DOTALL)
        json_str = json_str.strip()
        
        if json_str.startswith('{') and json_str.endswith('}'):
            pass
        else:
            start = json_str.find('{')
            end = json_str.rfind('}')
            if start != -1 and end != -1:
                json_str = json_str[start:end+1]
        
        try:
            data = json.loads(json_str)
            return LLMWorkflowPlan(
                analysis=data.get("analysis", ""),
                execution_plan=data.get("execution_plan", []),
                dependencies=data.get("dependencies", {}),
                raw_response=response
            )
        except json.JSONDecodeError as e:
            logger.warning(f"JSON è§£æå¤±è´¥: {e}, åŸå§‹å“åº”: {response[:200]}")
            return LLMWorkflowPlan(
                analysis="è§£æå¤±è´¥",
                execution_plan=[],
                dependencies={},
                raw_response=response
            )
    
    def _fallback_to_rule_planner(self, tool_calls: List[Dict]) -> LLMWorkflowPlan:
        """å›é€€åˆ°è§„åˆ™è§„åˆ’å™¨"""
        rule_plan = self.rule_planner.analyze_tool_calls(tool_calls)
        
        execution_plan = []
        for i, level in enumerate(rule_plan.execution_order):
            execution_plan.append({
                "step": i + 1,
                "parallel": len(level) > 1,
                "tools": level,
                "reason": "è§„åˆ™åˆ†æå™¨è§„åˆ’"
            })
        
        dependencies = {}
        for name, node in rule_plan.nodes.items():
            if node.dependencies:
                dependencies[name] = node.dependencies
        
        return LLMWorkflowPlan(
            analysis="ä½¿ç”¨è§„åˆ™åˆ†æå™¨è§„åˆ’",
            execution_plan=execution_plan,
            dependencies=dependencies,
            raw_response=""
        )
    
    def _get_default_tools_description(self) -> str:
        """è·å–é»˜è®¤å·¥å…·æè¿°"""
        return """
| å·¥å…·åç§° | åŠŸèƒ½ | è¾“å‡ºç±»å‹ | å¯æä¾› |
|---------|------|---------|--------|
| contact_list | è·å–é€šè®¯å½• | data | content, contacts |
| contact_lookup | æŸ¥æ‰¾è”ç³»äºº | data | content, contact_info |
| search_web | ç½‘ç»œæœç´¢ | data | content, search_result |
| get_weather | è·å–å¤©æ°” | data | content, weather_data |
| save_document | ä¿å­˜æ–‡æ¡£ | file_path | attachment, file_path |
| generate_image | ç”Ÿæˆå›¾ç‰‡ | file_path | attachment, image_path |
| send_email | å‘é€é‚®ä»¶ | status | æ—  |
| play_music | æ’­æ”¾éŸ³ä¹ | status | æ—  |
| system_control | ç³»ç»Ÿæ§åˆ¶ | status | æ—  |
| open_app | æ‰“å¼€åº”ç”¨ | status | æ—  |
"""
    
    def to_workflow_plan(self, llm_plan: LLMWorkflowPlan, tool_calls: List[Dict]) -> WorkflowPlan:
        """å°† LLM è§„åˆ’è½¬æ¢ä¸ºæ ‡å‡† WorkflowPlan"""
        tool_call_map = {tc["name"]: tc for tc in tool_calls}
        
        nodes = {}
        execution_order = []
        
        for step in llm_plan.execution_plan:
            level_tools = step.get("tools", [])
            execution_order.append(level_tools)
            
            for tool_name in level_tools:
                tc = tool_call_map.get(tool_name, {"name": tool_name, "arguments": {}})
                deps = llm_plan.dependencies.get(tool_name, [])
                
                nodes[tool_name] = WorkflowNode(
                    name=tool_name,
                    tool_name=tool_name,
                    arguments=tc.get("arguments", {}),
                    dependencies=deps,
                    execution_mode=ExecutionMode.PARALLEL if step.get("parallel") else ExecutionMode.SEQUENTIAL
                )
        
        return WorkflowPlan(nodes=nodes, execution_order=execution_order)


async def create_llm_workflow_planner() -> LLMWorkflowPlanner:
    """åˆ›å»º LLM å·¥ä½œæµè§„åˆ’å™¨å®ä¾‹"""
    return LLMWorkflowPlanner()
