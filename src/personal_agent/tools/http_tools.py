"""
HTTP Request Tool - Simple web requests without Playwright
"""
import json
from typing import Optional, Dict, Any, List
from urllib.parse import quote_plus

from .base import BaseTool, ToolResult, tool_registry

REQUESTS_AVAILABLE = False
try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    pass

BEAUTIFULSOUP_AVAILABLE = False
try:
    from bs4 import BeautifulSoup
    BEAUTIFULSOUP_AVAILABLE = True
except ImportError:
    pass


class HttpRequestTool(BaseTool):
    name = "http_request"
    description = "å‘é€HTTPè¯·æ±‚è·å–ç½‘é¡µå†…å®¹æˆ–APIæ•°æ®"
    parameters = {
        "type": "object",
        "properties": {
            "url": {
                "type": "string",
                "description": "è¯·æ±‚çš„URL"
            },
            "method": {
                "type": "string",
                "enum": ["GET", "POST"],
                "default": "GET",
                "description": "HTTPæ–¹æ³•"
            },
            "headers": {
                "type": "object",
                "description": "è¯·æ±‚å¤´ï¼ˆå¯é€‰ï¼‰"
            },
            "data": {
                "type": "object",
                "description": "POSTæ•°æ®ï¼ˆå¯é€‰ï¼‰"
            },
            "timeout": {
                "type": "integer",
                "default": 30,
                "description": "è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰"
            }
        },
        "required": ["url"]
    }

    async def execute(
        self,
        url: str,
        method: str = "GET",
        headers: Optional[Dict] = None,
        data: Optional[Dict] = None,
        timeout: int = 30
    ) -> ToolResult:
        if not REQUESTS_AVAILABLE:
            return ToolResult(
                success=False,
                output="",
                error="requestsåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install requests"
            )
        
        default_headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        }
        if headers:
            default_headers.update(headers)

        try:
            if method.upper() == "GET":
                response = requests.get(url, headers=default_headers, timeout=timeout)
            else:
                response = requests.post(url, headers=default_headers, json=data, timeout=timeout)
            
            response.raise_for_status()
            
            content_type = response.headers.get("Content-Type", "")
            if "application/json" in content_type:
                result = response.json()
                return ToolResult(
                    success=True,
                    output=json.dumps(result, ensure_ascii=False, indent=2)[:5000],
                    data={"status_code": response.status_code, "json": result}
                )
            else:
                text = response.text
                return ToolResult(
                    success=True,
                    output=text[:10000],
                    data={"status_code": response.status_code, "length": len(text)}
                )

        except requests.exceptions.Timeout:
            return ToolResult(success=False, output="", error=f"è¯·æ±‚è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰")
        except requests.exceptions.RequestException as e:
            return ToolResult(success=False, output="", error=f"è¯·æ±‚å¤±è´¥: {str(e)}")


class WebFetchTool(BaseTool):
    name = "web_fetch"
    description = "è·å–ç½‘é¡µå†…å®¹å¹¶æå–æ­£æ–‡æ–‡æœ¬"
    parameters = {
        "type": "object",
        "properties": {
            "url": {
                "type": "string",
                "description": "ç½‘é¡µURL"
            },
            "selector": {
                "type": "string",
                "description": "CSSé€‰æ‹©å™¨ï¼Œæå–ç‰¹å®šå…ƒç´ ï¼ˆå¯é€‰ï¼‰"
            }
        },
        "required": ["url"]
    }

    async def execute(self, url: str, selector: Optional[str] = None) -> ToolResult:
        if not REQUESTS_AVAILABLE:
            return ToolResult(
                success=False,
                output="",
                error="requestsåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install requests"
            )

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        }

        try:
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()

            if BEAUTIFULSOUP_AVAILABLE:
                soup = BeautifulSoup(response.text, "html.parser")
                
                for script in soup(["script", "style", "nav", "footer", "header"]):
                    script.decompose()
                
                if selector:
                    elements = soup.select(selector)
                    text = "\n".join(el.get_text(strip=True) for el in elements)
                else:
                    text = soup.get_text(separator="\n", strip=True)
                    lines = [line.strip() for line in text.split("\n") if line.strip()]
                    text = "\n".join(lines)
            else:
                text = response.text

            return ToolResult(
                success=True,
                output=text[:8000],
                data={"url": url, "length": len(text)}
            )

        except Exception as e:
            return ToolResult(success=False, output="", error=f"è·å–ç½‘é¡µå¤±è´¥: {str(e)}")


class WebSearchTool(BaseTool):
    name = "web_search"
    description = "ä½¿ç”¨æœç´¢å¼•æ“æœç´¢ä¿¡æ¯ï¼ˆä½¿ç”¨DuckDuckGoï¼Œæ— éœ€APIå¯†é’¥ï¼‰"
    parameters = {
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "æœç´¢å…³é”®è¯"
            },
            "max_results": {
                "type": "integer",
                "default": 5,
                "description": "æœ€å¤§ç»“æœæ•°"
            }
        },
        "required": ["query"]
    }

    async def execute(self, query: str, max_results: int = 5) -> ToolResult:
        if not REQUESTS_AVAILABLE:
            return ToolResult(
                success=False,
                output="",
                error="requestsåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install requests"
            )

        try:
            url = f"https://html.duckduckgo.com/html/?q={quote_plus(query)}"
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            }
            
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()

            results = []
            if BEAUTIFULSOUP_AVAILABLE:
                soup = BeautifulSoup(response.text, "html.parser")
                for result in soup.select(".result")[:max_results]:
                    title_elem = result.select_one(".result__a")
                    snippet_elem = result.select_one(".result__snippet")
                    if title_elem:
                        results.append({
                            "title": title_elem.get_text(strip=True),
                            "url": title_elem.get("href", ""),
                            "snippet": snippet_elem.get_text(strip=True) if snippet_elem else ""
                        })
            else:
                results = [{"title": "è¯·å®‰è£…beautifulsoup4ä»¥è·å¾—æ›´å¥½çš„è§£æ", "url": "", "snippet": ""}]

            if not results:
                return ToolResult(
                    success=True,
                    output=f"æœªæ‰¾åˆ°å…³äº '{query}' çš„ç»“æœ",
                    data={"query": query, "results": []}
                )

            output_lines = []
            for i, r in enumerate(results, 1):
                output_lines.append(f"ã€{i}ã€‘{r['title']}")
                if r['snippet']:
                    output_lines.append(f"   {r['snippet']}")
                if r['url']:
                    output_lines.append(f"   é“¾æ¥: {r['url']}")
                output_lines.append("")

            return ToolResult(
                success=True,
                output="\n".join(output_lines),
                data={"query": query, "results": results}
            )

        except Exception as e:
            return ToolResult(success=False, output="", error=f"æœç´¢å¤±è´¥: {str(e)}")


class NewsFetchTool(BaseTool):
    name = "news_fetch"
    description = "è·å–æœ€æ–°æ–°é—»èµ„è®¯ï¼ˆä»RSSæºæˆ–æ–°é—»ç½‘ç«™ï¼‰"
    parameters = {
        "type": "object",
        "properties": {
            "category": {
                "type": "string",
                "enum": ["general", "tech", "finance", "world"],
                "default": "general",
                "description": "æ–°é—»ç±»åˆ«"
            },
            "max_items": {
                "type": "integer",
                "default": 10,
                "description": "æœ€å¤§æ–°é—»æ¡æ•°"
            }
        },
        "required": []
    }

    NEWS_SOURCES = {
        "general": [
            "https://news.qq.com/newsgn/rss_newsgn.xml",
            "https://feedx.net/rss/zgxwzk.xml",
        ],
        "tech": [
            "https://www.36kr.com/feed",
            "https://www.ifanr.com/feed",
        ],
        "finance": [
            "https://feedx.net/rss/caijingjie.xml",
        ],
        "world": [
            "https://feedx.net/rss/cankaoxiaoxi.xml",
        ]
    }

    async def execute(self, category: str = "general", max_items: int = 10) -> ToolResult:
        if not REQUESTS_AVAILABLE:
            return ToolResult(
                success=False,
                output="",
                error="requestsåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install requests"
            )

        all_news = []
        sources = self.NEWS_SOURCES.get(category, self.NEWS_SOURCES["general"])
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "application/rss+xml,application/xml,text/xml",
        }

        for source_url in sources:
            try:
                response = requests.get(source_url, headers=headers, timeout=15)
                response.raise_for_status()
                
                if BEAUTIFULSOUP_AVAILABLE:
                    soup = BeautifulSoup(response.text, "xml")
                    items = soup.find_all("item")
                    
                    for item in items[:max_items]:
                        title = item.find("title")
                        link = item.find("link")
                        desc = item.find("description")
                        pub_date = item.find("pubDate")
                        
                        if title:
                            all_news.append({
                                "title": title.get_text(strip=True),
                                "link": link.get_text(strip=True) if link else "",
                                "description": desc.get_text(strip=True)[:200] if desc else "",
                                "date": pub_date.get_text(strip=True) if pub_date else ""
                            })
            except Exception:
                continue

        if not all_news:
            return ToolResult(
                success=True,
                output="æš‚æ—¶æ— æ³•è·å–æ–°é—»ï¼Œè¯·ç¨åå†è¯•",
                data={"category": category, "news": []}
            )

        all_news = all_news[:max_items]
        
        output_lines = [f"ğŸ“° {category.upper()} æ–°é—»èµ„è®¯\n"]
        for i, news in enumerate(all_news, 1):
            output_lines.append(f"ã€{i}ã€‘{news['title']}")
            if news['description']:
                output_lines.append(f"    {news['description']}")
            output_lines.append("")

        return ToolResult(
            success=True,
            output="\n".join(output_lines),
            data={"category": category, "news": all_news, "count": len(all_news)}
        )


def register_http_tools():
    tool_registry.register(HttpRequestTool())
    tool_registry.register(WebFetchTool())
    tool_registry.register(WebSearchTool())
    tool_registry.register(NewsFetchTool())
