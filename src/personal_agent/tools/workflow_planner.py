"""
å·¥ä½œæµè§„åˆ’å™¨ - åˆ†æä»»åŠ¡ä¾èµ–å…³ç³»ï¼Œæ„å»ºæ‰§è¡Œè®¡åˆ’

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. åˆ†æå·¥å…·ä¹‹é—´çš„ä¾èµ–å…³ç³»
2. æ„å»ºå·¥ä½œæµå›¾ï¼ˆDAGï¼‰
3. åˆ¤æ–­å“ªäº›ä»»åŠ¡å¯ä»¥å¹¶è¡Œï¼Œå“ªäº›å¿…é¡»ä¸²è¡Œ
4. ç”Ÿæˆæ‰§è¡Œè®¡åˆ’

é€šç”¨æ€§è®¾è®¡ï¼š
- å·¥å…·ä¾èµ–è§„åˆ™å¯é…ç½®
- å‚æ•°ä¼ é€’åŸºäºç±»å‹åŒ¹é…
- æ”¯æŒå¹¶è¡Œå’Œä¸²è¡Œæ‰§è¡Œ
- æ”¯æŒåŠ¨æ€ä¾èµ–æ¨æ–­
"""
from typing import Dict, List, Set, Tuple, Any, Optional, Callable
from dataclasses import dataclass, field
from enum import Enum
from loguru import logger
import json
import re


class ExecutionMode(Enum):
    SEQUENTIAL = "sequential"
    PARALLEL = "parallel"


@dataclass
class WorkflowNode:
    name: str
    tool_name: str
    arguments: Dict[str, Any]
    dependencies: List[str] = field(default_factory=list)
    output_key: str = ""
    execution_mode: ExecutionMode = ExecutionMode.SEQUENTIAL


@dataclass
class WorkflowPlan:
    nodes: Dict[str, WorkflowNode] = field(default_factory=dict)
    execution_order: List[List[str]] = field(default_factory=list)
    
    def is_empty(self) -> bool:
        return len(self.nodes) == 0
    
    def get_node(self, name: str) -> Optional[WorkflowNode]:
        return self.nodes.get(name)


@dataclass
class ToolDependency:
    name: str
    output_type: str
    can_provide: List[str]
    requires_input: List[str]
    description: str


class WorkflowPlanner:
    
    DEFAULT_TOOL_INFO: Dict[str, Dict] = {
        "save_document": {
            "output_type": "file_path",
            "can_provide": ["attachment", "file_path"],
            "requires_input": ["content"],
            "description": "ç”Ÿæˆæ–‡æ¡£æ–‡ä»¶"
        },
        "generate_image": {
            "output_type": "file_path",
            "can_provide": ["attachment", "file_path", "image_path"],
            "requires_input": [],
            "description": "ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶"
        },
        "create_travel_plan": {
            "output_type": "data",
            "can_provide": ["content", "data", "travel_plan"],
            "requires_input": [],
            "description": "ç”Ÿæˆæ—…æ¸¸æ”»ç•¥"
        },
        "contact_list": {
            "output_type": "data",
            "can_provide": ["content", "data", "contacts"],
            "requires_input": [],
            "description": "è·å–è”ç³»äººæ•°æ®"
        },
        "contact_lookup": {
            "output_type": "data",
            "can_provide": ["content", "data", "contact_info"],
            "requires_input": [],
            "description": "æŸ¥æ‰¾è”ç³»äººä¿¡æ¯"
        },
        "search_web": {
            "output_type": "data",
            "can_provide": ["content", "data", "search_result"],
            "requires_input": [],
            "description": "æœç´¢ç½‘ç»œæ•°æ®"
        },
        "crawl_webpage": {
            "output_type": "data",
            "can_provide": ["content", "data", "crawl_result"],
            "requires_input": [],
            "description": "çˆ¬å–ç½‘é¡µæ•°æ®"
        },
        "search": {
            "output_type": "data",
            "can_provide": ["content", "data", "search_result"],
            "requires_input": [],
            "description": "æœç´¢æ•°æ®"
        },
        "send_email": {
            "output_type": "status",
            "can_provide": [],
            "requires_input": ["attachment", "file_path"],
            "description": "å‘é€é‚®ä»¶"
        },
        "play_music": {
            "output_type": "status",
            "can_provide": [],
            "requires_input": [],
            "description": "æ’­æ”¾éŸ³ä¹"
        },
        "get_weather": {
            "output_type": "data",
            "can_provide": ["content", "weather_data"],
            "requires_input": [],
            "description": "è·å–å¤©æ°”"
        },
        "get_news": {
            "output_type": "data",
            "can_provide": ["content", "data", "news"],
            "requires_input": [],
            "description": "è·å–æ–°é—»"
        },
        "system_control": {
            "output_type": "status",
            "can_provide": [],
            "requires_input": [],
            "description": "ç³»ç»Ÿæ§åˆ¶"
        },
        "open_app": {
            "output_type": "status",
            "can_provide": [],
            "requires_input": [],
            "description": "æ‰“å¼€åº”ç”¨"
        },
    }
    
    def __init__(self, custom_tool_info: Optional[Dict[str, Dict]] = None):
        self.tool_info = dict(self.DEFAULT_TOOL_INFO)
        if custom_tool_info:
            self.tool_info.update(custom_tool_info)
    
    def register_tool(self, name: str, output_type: str, can_provide: List[str], 
                      requires_input: List[str], description: str = ""):
        """åŠ¨æ€æ³¨å†Œå·¥å…·ä¿¡æ¯"""
        self.tool_info[name] = {
            "output_type": output_type,
            "can_provide": can_provide,
            "requires_input": requires_input,
            "description": description
        }
    
    def analyze_tool_calls(self, tool_calls: List[Dict]) -> WorkflowPlan:
        """
        åˆ†æå·¥å…·è°ƒç”¨åˆ—è¡¨ï¼Œæ„å»ºå·¥ä½œæµè®¡åˆ’
        
        Args:
            tool_calls: LLM è¿”å›çš„å·¥å…·è°ƒç”¨åˆ—è¡¨
            
        Returns:
            WorkflowPlan: å·¥ä½œæµæ‰§è¡Œè®¡åˆ’
        """
        if not tool_calls:
            return WorkflowPlan()
        
        if len(tool_calls) == 1:
            node = WorkflowNode(
                name=tool_calls[0]["name"],
                tool_name=tool_calls[0]["name"],
                arguments=tool_calls[0]["arguments"],
                execution_mode=ExecutionMode.SEQUENTIAL
            )
            plan = WorkflowPlan()
            plan.nodes[node.name] = node
            plan.execution_order = [[node.name]]
            return plan
        
        nodes = {}
        for i, tc in enumerate(tool_calls):
            node_name = f"{tc['name']}_{i}" if len([t for t in tool_calls if t['name'] == tc['name']]) > 1 else tc['name']
            node = WorkflowNode(
                name=node_name,
                tool_name=tc["name"],
                arguments=tc["arguments"],
                execution_mode=ExecutionMode.SEQUENTIAL
            )
            nodes[node_name] = node
        
        self._analyze_dependencies(nodes)
        
        execution_order = self._topological_sort(nodes)
        
        return WorkflowPlan(nodes=nodes, execution_order=execution_order)
    
    def _analyze_dependencies(self, nodes: Dict[str, WorkflowNode]):
        """åˆ†æèŠ‚ç‚¹ä¹‹é—´çš„ä¾èµ–å…³ç³»"""
        node_list = list(nodes.values())
        
        for i, node in enumerate(node_list):
            tool_info = self.tool_info.get(node.tool_name, {})
            requires_input = tool_info.get("requires_input", [])
            
            for input_type in requires_input:
                for j in range(i):
                    prev_node = node_list[j]
                    prev_info = self.tool_info.get(prev_node.tool_name, {})
                    can_provide = prev_info.get("can_provide", [])
                    
                    if input_type in can_provide:
                        if prev_node.name not in node.dependencies:
                            node.dependencies.append(prev_node.name)
            
            self._analyze_fake_path_dependency(node, node_list, i)
            
            self._analyze_empty_param_dependency(node, node_list, i)
    
    def _analyze_fake_path_dependency(self, node: WorkflowNode, node_list: List[WorkflowNode], current_idx: int):
        """åˆ†æç¼–é€ è·¯å¾„çš„ä¾èµ–"""
        for param_name in ["attachment", "file_path", "image_path"]:
            param_value = node.arguments.get(param_name, "")
            if param_value and self._is_fake_path(param_value):
                for j in range(current_idx):
                    prev_node = node_list[j]
                    prev_info = self.tool_info.get(prev_node.tool_name, {})
                    if prev_info.get("output_type") == "file_path":
                        if prev_node.name not in node.dependencies:
                            node.dependencies.append(prev_node.name)
    
    def _analyze_empty_param_dependency(self, node: WorkflowNode, node_list: List[WorkflowNode], current_idx: int):
        """åˆ†æç©ºå‚æ•°çš„ä¾èµ–"""
        tool_info = self.tool_info.get(node.tool_name, {})
        requires_input = tool_info.get("requires_input", [])
        
        for param_name in ["content", "data"]:
            if param_name not in requires_input:
                continue
            param_value = node.arguments.get(param_name, "")
            if param_name not in node.arguments or not param_value or param_value == "{}" or "{previous" in str(param_value):
                for j in range(current_idx):
                    prev_node = node_list[j]
                    prev_info = self.tool_info.get(prev_node.tool_name, {})
                    can_provide = prev_info.get("can_provide", [])
                    if prev_info.get("output_type") == "data" or param_name in can_provide:
                        if prev_node.name not in node.dependencies:
                            node.dependencies.append(prev_node.name)
    
    def _is_fake_path(self, path: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯ç¼–é€ çš„è·¯å¾„"""
        fake_patterns = [
            "/path/to/",
            "/path/",
            "\\path\\",
            "./output/",
            "output/xxx",
            "[å¾…å®š]",
            "[é™„ä»¶]",
            "[æ–‡ä»¶]",
            "{attachment}",
            "{file_path}",
        ]
        path_lower = path.lower()
        for pattern in fake_patterns:
            if pattern.lower() in path_lower:
                return True
        if not re.search(r'[A-Za-z]:\\', path) and not path.startswith("/"):
            return True
        return False
    
    def _topological_sort(self, nodes: Dict[str, WorkflowNode]) -> List[List[str]]:
        """
        æ‹“æ‰‘æ’åºï¼Œç”Ÿæˆæ‰§è¡Œé¡ºåº
        
        Returns:
            List[List[str]]: æ¯ä¸ªå†…å±‚åˆ—è¡¨æ˜¯å¯ä»¥å¹¶è¡Œæ‰§è¡Œçš„èŠ‚ç‚¹
        """
        if not nodes:
            return []
        
        in_degree = {name: 0 for name in nodes}
        for node in nodes.values():
            for dep in node.dependencies:
                if dep in in_degree:
                    in_degree[node.name] += 1
        
        execution_order = []
        remaining = set(nodes.keys())
        max_iterations = len(nodes) + 1
        iteration = 0
        
        while remaining and iteration < max_iterations:
            iteration += 1
            ready = [name for name in remaining if in_degree[name] == 0]
            
            if not ready:
                cycle_nodes = self._detect_cycle_nodes(nodes, remaining)
                if cycle_nodes:
                    logger.warning(f"âš ï¸ æ£€æµ‹åˆ°å¾ªç¯ä¾èµ–: {cycle_nodes}")
                    self._break_cycle(nodes, cycle_nodes)
                    for name in remaining:
                        in_degree[name] = sum(1 for dep in nodes[name].dependencies if dep in remaining)
                    continue
                else:
                    logger.warning("âš ï¸ æ— æ³•è§£å†³ä¾èµ–é—®é¢˜ï¼Œå¼ºåˆ¶æ‰§è¡Œå‰©ä½™èŠ‚ç‚¹")
                    ready = list(remaining)
            
            execution_order.append(ready)
            
            for name in ready:
                remaining.remove(name)
                for node in nodes.values():
                    if name in node.dependencies:
                        in_degree[node.name] -= 1
        
        return execution_order
    
    def _detect_cycle_nodes(self, nodes: Dict[str, WorkflowNode], remaining: Set[str]) -> List[str]:
        """æ£€æµ‹å‚ä¸å¾ªç¯çš„èŠ‚ç‚¹"""
        def dfs(node_name: str, visited: Set[str], rec_stack: Set[str], path: List[str]) -> Optional[List[str]]:
            visited.add(node_name)
            rec_stack.add(node_name)
            path.append(node_name)
            
            node = nodes.get(node_name)
            if node:
                for dep in node.dependencies:
                    if dep in remaining:
                        if dep not in visited:
                            result = dfs(dep, visited, rec_stack, path)
                            if result:
                                return result
                        elif dep in rec_stack:
                            cycle_start = path.index(dep)
                            return path[cycle_start:]
            
            path.pop()
            rec_stack.remove(node_name)
            return None
        
        visited: Set[str] = set()
        for node_name in remaining:
            if node_name not in visited:
                cycle = dfs(node_name, visited, set(), [])
                if cycle:
                    return cycle
        return []
    
    def _break_cycle(self, nodes: Dict[str, WorkflowNode], cycle_nodes: List[str]) -> None:
        """æ‰“ç ´å¾ªç¯ä¾èµ–"""
        if len(cycle_nodes) >= 2:
            first_node = nodes.get(cycle_nodes[0])
            second_node_name = cycle_nodes[1]
            if first_node and second_node_name in first_node.dependencies:
                first_node.dependencies.remove(second_node_name)
                logger.info(f"ğŸ”§ æ‰“ç ´å¾ªç¯ä¾èµ–: ç§»é™¤ {first_node.name} å¯¹ {second_node_name} çš„ä¾èµ–")
    
    def can_execute_parallel(self, plan: WorkflowPlan) -> bool:
        """åˆ¤æ–­æ˜¯å¦æœ‰å¯å¹¶è¡Œæ‰§è¡Œçš„ä»»åŠ¡"""
        return any(len(level) > 1 for level in plan.execution_order)
    
    def get_execution_summary(self, plan: WorkflowPlan) -> str:
        """è·å–æ‰§è¡Œè®¡åˆ’æ‘˜è¦"""
        if plan.is_empty():
            return "æ— ä»»åŠ¡"
        
        lines = ["ğŸ“‹ å·¥ä½œæµæ‰§è¡Œè®¡åˆ’:"]
        for i, level in enumerate(plan.execution_order):
            if len(level) == 1:
                lines.append(f"  æ­¥éª¤{i+1}: {level[0]}")
            else:
                lines.append(f"  æ­¥éª¤{i+1}: [å¹¶è¡Œ] {', '.join(level)}")
        
        return "\n".join(lines)


def create_workflow_planner(custom_tool_info: Optional[Dict[str, Dict]] = None) -> WorkflowPlanner:
    """åˆ›å»ºå·¥ä½œæµè§„åˆ’å™¨å®ä¾‹"""
    return WorkflowPlanner(custom_tool_info)
